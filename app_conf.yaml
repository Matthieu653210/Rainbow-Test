################
# Application configuration file 
################

# cSpell:disable

# https://omegaconf.readthedocs.io/en/2.3_branch/usage.html#variable-interpolation


paths:
  project : ${oc.env:PWD}
  data_root : ${oc.env:HOME}

# Default configuration
baseline:
  llm:
    list: ${paths.project}/models_providers.yaml
    default_model: gpt_4omini_openai # gpt_35_azure # mistral_large_edenai 
    cache : sqlite
    cache_path: ${paths.data_root}/llm_cache/langchain_ecod.db

    # optional model definitions (you can add your own type): 
    coder_model : qwen25_coder32_openrouter
    powerful_model : gpt_4_openai
    fast_model : llama31_8_groq
    
  embeddings:
    list: ${paths.project}/models_providers.yaml
    default_model:  multilingual_MiniLM_local #ada_002_edenai #
    cache: ${paths.data_root}/hf_models
  vector_store:
    default: Chroma
    path: ${paths.data_root}/vector_store
    default_collection: "training_session"
  documents:
    base: ./use_case_data
  monitoring: 
    default:  none
    project: GenAI_demo

  kv_store:
    path: ${paths.data_root}/kv_store

  commands:
    modules:
      - src.ai_core.cli_commands

  chains:
    root: src.ai_chains
    modules:
      - ${..root}.A_1_joke 
      # - ${..root}.B_1_naive_rag_example
      # - ${..root}.B_2_self_query
      # - ${..root}.C_1_tools_example
      # - ${..root}.C_2_advanced_rag_langgraph
      # - ${..root}.C_3_essay_writer_agent
      
# Overridden configurations
# Set by environment variable "BLUEPRINT_CONFIG", or by the 'default' key, or in code


training_local:
  llm:
    default_model:  llama32_3_ollama
  embeddings:
    default_model: artic_22_ollama

training_edenai:
  llm:
    default_model: gpt_4omini_edenai
    cache : memory
  embeddings:
    default_model: ada_002_edenai

training_azure:
  llm:
    default_model: gpt_4o_azure
  embeddings:
    default_model: ada_002_azure

default_config: training_edenai   # can be overidden by 'BLUEPRINT_CONFIG' environment variable
