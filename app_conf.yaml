
# Baseline configuration
baseline:
  llm:
    list: ${PWD}/models_providers.yaml
    cache : no_cache
    cache_path: ${HOME}/cache_ai/langchain.db
    default_model: gpt_4omini_openai # gpt_35_azure # mistral_large_edenai 

    # optional model definitions (you can add your orn type): 
    coder_model : qwen25_coder32_openrouter
    powerful_model : gpt_4_openai
    fast_model : llama31_8_groq

  embeddings:
    list: ${PWD}/models_providers.yaml
    default_model:  multilingual_MiniLM_local #ada_002_edenai #
    cache: ${HOME}/hf_models
  vector_store:
    default: Chroma
    path: ${HOME}/vector_store
    default_collection: "training_session"
  documents:
    base: ./use_case_data
  monitoring: 
    default:  langsmith  # none #
    project: GenAI_demo
  kv_store:
    default : LocalFileStore  # no alternative yet
    path : ${HOME}/kv_store

  chains:
    path: python.ai_chains
    modules:
      - A_1_joke 
      - B_1_naive_rag_example
      - B_2_self_query
      - C_1_tools_example
      - C_2_advanced_rag_langgraph
      - C_3_essay_writer_agent


# Overridden configurations
# Set by environment variable "BLUEPRINT_CONFIG", or by the 'default' key, or in code

pytest:
  llm: 
    default_model : gpt_4omini_openai
    model2 : llama31_8_deepinfra
    cache : in_memory
  embeddings:
    default_model:  ada_002_openai

training_local:
  llm:
    default_model:  llama32_3_ollama
  embeddings:
    default_model: artic_22_ollama

training_edenai:
  llm:
    default_model: gpt_4omini_edenai
    cache : memory
  embeddings:
    default_model: ada_002_edenai

training_azure:
  llm:
    default_model: gpt_4_azure
  embeddings:
    default_model: ada_002_azure


# default_config: training_azure   # can be overidden by 'BLUEPRINT_CONFIG' environment variable