{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Joke (First LLM calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from devtools import debug\n",
    "\n",
    "!export PYTHONPATH=\":./python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Factory\n",
    "\n",
    "To facilitate the selection and configuration of LLM and their provider, we have an \"LLM Factory'. <br> <br>\n",
    "See [llm.py](../python/ai_core/llm.py)  <br>\n",
    "\n",
    "List of hard-coded LLM configuration is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt_35_openai': LLM_INFO(id='gpt_35_openai', cls='ChatOpenAI', model='gpt-3.5-turbo-0125', key='OPENAI_API_KEY', agent_builder_type='tool_calling', agent_builder=AgentBuilder(type='tool_calling', create_function=<function create_tool_calling_agent at 0x7f74f3bed7e0>, hub_prompt='hwchase17/openai-tools-agent')),\n",
       " 'gpt_4o_openai': LLM_INFO(id='gpt_4o_openai', cls='ChatOpenAI', model='gpt-4o', key='OPENAI_API_KEY', agent_builder_type='tool_calling', agent_builder=AgentBuilder(type='tool_calling', create_function=<function create_tool_calling_agent at 0x7f74f3bed7e0>, hub_prompt='hwchase17/openai-tools-agent')),\n",
       " 'llama3_70_deepinfra': LLM_INFO(id='llama3_70_deepinfra', cls='ChatDeepInfra', model='meta-llama/Meta-Llama-3-70B-Instruct', key='DEEPINFRA_API_TOKEN', agent_builder_type='tool_calling', agent_builder=AgentBuilder(type='tool_calling', create_function=<function create_tool_calling_agent at 0x7f74f3bed7e0>, hub_prompt='hwchase17/openai-tools-agent')),\n",
       " 'llama3_8_deepinfra': LLM_INFO(id='llama3_8_deepinfra', cls='ChatDeepInfra', model='meta-llama/Meta-Llama-3-8B-Instruct', key='DEEPINFRA_API_TOKEN', agent_builder_type='tool_calling', agent_builder=AgentBuilder(type='tool_calling', create_function=<function create_tool_calling_agent at 0x7f74f3bed7e0>, hub_prompt='hwchase17/openai-tools-agent')),\n",
       " 'mixtral_7x8_deepinfra': LLM_INFO(id='mixtral_7x8_deepinfra', cls='ChatDeepInfra', model='mistralai/Mixtral-8x7B-Instruct-v0.1', key='DEEPINFRA_API_TOKEN', agent_builder_type='tool_calling', agent_builder=AgentBuilder(type='tool_calling', create_function=<function create_tool_calling_agent at 0x7f74f3bed7e0>, hub_prompt='hwchase17/openai-tools-agent')),\n",
       " 'llama3_70_deepinfra_lite': LLM_INFO(id='llama3_70_deepinfra_lite', cls='ChatLiteLLM', model='deepinfra/meta-llama/Llama-3-70b-chat-hf', key='DEEPINFRA_API_TOKEN', agent_builder_type='tool_calling', agent_builder=AgentBuilder(type='tool_calling', create_function=<function create_tool_calling_agent at 0x7f74f3bed7e0>, hub_prompt='hwchase17/openai-tools-agent')),\n",
       " 'llama3_70_groq': LLM_INFO(id='llama3_70_groq', cls='ChatGroq', model='lLama3-70b-8192', key='GROQ_API_KEY', agent_builder_type='tool_calling', agent_builder=AgentBuilder(type='tool_calling', create_function=<function create_tool_calling_agent at 0x7f74f3bed7e0>, hub_prompt='hwchase17/openai-tools-agent')),\n",
       " 'llama3_8_groq': LLM_INFO(id='llama3_8_groq', cls='ChatGroq', model='lLama3-8b-8192', key='GROQ_API_KEY', agent_builder_type='tool_calling', agent_builder=AgentBuilder(type='tool_calling', create_function=<function create_tool_calling_agent at 0x7f74f3bed7e0>, hub_prompt='hwchase17/openai-tools-agent')),\n",
       " 'mixtral_7x8_groq': LLM_INFO(id='mixtral_7x8_groq', cls='ChatGroq', model='Mixtral-8x7b-32768', key='GROQ_API_KEY', agent_builder_type='tool_calling', agent_builder=AgentBuilder(type='tool_calling', create_function=<function create_tool_calling_agent at 0x7f74f3bed7e0>, hub_prompt='hwchase17/openai-tools-agent')),\n",
       " 'gemini_pro_google': LLM_INFO(id='gemini_pro_google', cls='ChatVertexAI', model='gemini-pro', key='GOOGLE_API_KEY', agent_builder_type='tool_calling', agent_builder=AgentBuilder(type='tool_calling', create_function=<function create_tool_calling_agent at 0x7f74f3bed7e0>, hub_prompt='hwchase17/openai-tools-agent')),\n",
       " 'llama3_8_local': LLM_INFO(id='llama3_8_local', cls='ChatOllama', model='llama3:instruct', key='', agent_builder_type='tool_calling', agent_builder=AgentBuilder(type='tool_calling', create_function=<function create_tool_calling_agent at 0x7f74f3bed7e0>, hub_prompt='hwchase17/openai-tools-agent')),\n",
       " 'gpt_4o_edenai': LLM_INFO(id='gpt_4o_edenai', cls='ChatEdenAI', model='openai/gpt-4o', key='EDENAI_API_KEY', agent_builder_type='tool_calling', agent_builder=AgentBuilder(type='tool_calling', create_function=<function create_tool_calling_agent at 0x7f74f3bed7e0>, hub_prompt='hwchase17/openai-tools-agent')),\n",
       " 'gpt_4_edenai': LLM_INFO(id='gpt_4_edenai', cls='ChatEdenAI', model='openai/gpt-4', key='EDENAI_API_KEY', agent_builder_type='tool_calling', agent_builder=AgentBuilder(type='tool_calling', create_function=<function create_tool_calling_agent at 0x7f74f3bed7e0>, hub_prompt='hwchase17/openai-tools-agent')),\n",
       " 'gpt_35_edenai': LLM_INFO(id='gpt_35_edenai', cls='ChatEdenAI', model='openai/gpt-3.5-turbo-0125', key='EDENAI_API_KEY', agent_builder_type='tool_calling', agent_builder=AgentBuilder(type='tool_calling', create_function=<function create_tool_calling_agent at 0x7f74f3bed7e0>, hub_prompt='hwchase17/openai-tools-agent')),\n",
       " 'mistral_large_edenai': LLM_INFO(id='mistral_large_edenai', cls='ChatEdenAI', model='mistral/large-latest', key='EDENAI_API_KEY', agent_builder_type='tool_calling', agent_builder=AgentBuilder(type='tool_calling', create_function=<function create_tool_calling_agent at 0x7f74f3bed7e0>, hub_prompt='hwchase17/openai-tools-agent')),\n",
       " 'gpt_4_azure': LLM_INFO(id='gpt_4_azure', cls='AzureChatOpenAI', model='gpt4-turbo/2023-05-15', key='AZURE_OPENAI_API_KEY=your_azure_openai_key_here', agent_builder_type='tool_calling', agent_builder=AgentBuilder(type='tool_calling', create_function=<function create_tool_calling_agent at 0x7f74f3bed7e0>, hub_prompt='hwchase17/openai-tools-agent')),\n",
       " 'gpt_35_azure': LLM_INFO(id='gpt_35_azure', cls='AzureChatOpenAI', model='gpt-35-turbo/2023-05-15', key='AZURE_OPENAI_API_KEY=your_azure_openai_key_here', agent_builder_type='tool_calling', agent_builder=AgentBuilder(type='tool_calling', create_function=<function create_tool_calling_agent at 0x7f74f3bed7e0>, hub_prompt='hwchase17/openai-tools-agent')),\n",
       " 'gpt_4o_azure': LLM_INFO(id='gpt_4o_azure', cls='AzureChatOpenAI', model='gpt-4o/2023-05-15', key='AZURE_OPENAI_API_KEY=your_azure_openai_key_here', agent_builder_type='tool_calling', agent_builder=AgentBuilder(type='tool_calling', create_function=<function create_tool_calling_agent at 0x7f74f3bed7e0>, hub_prompt='hwchase17/openai-tools-agent'))}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from python.ai_core.llm import LlmFactory, get_llm\n",
    "\n",
    "LlmFactory().known_items_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of an LLM and LLM provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-26 23:53:07.980\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpython.ai_core.llm\u001b[0m:\u001b[36mget_llm\u001b[0m:\u001b[36m395\u001b[0m - \u001b[1mget LLM : gpt_35_edenai - configurable: True\u001b[0m\n",
      "\u001b[32m2024-06-26 23:53:08.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpython.ai_core.llm\u001b[0m:\u001b[36mget_configurable\u001b[0m:\u001b[36m351\u001b[0m - \u001b[1mCannot load gemini_pro_google: No module named 'langchain_google_vertexai'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_322719/1636673568.py:6 <module>\n",
      "    llm_default: RunnableConfigurableAlternatives(\n",
      "        default=ChatEdenAI(\n",
      "            model='gpt-3.5-turbo-0125',\n",
      "            max_tokens=2048,\n",
      "            edenai_api_key=SecretStr('**********'),\n",
      "        ),\n",
      "        which=ConfigurableField(\n",
      "            id='llm',\n",
      "            name=None,\n",
      "            description=None,\n",
      "            annotation=None,\n",
      "            is_shared=False,\n",
      "        ),\n",
      "        alternatives={\n",
      "            'gpt_35_openai': ChatOpenAI(\n",
      "                client=<openai.resources.chat.completions.Completions object at 0x7f750ff10250>,\n",
      "                async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f750ff11c90>,\n",
      "                model_name='gpt-3.5-turbo-0125',\n",
      "                temperature=0.0,\n",
      "                model_kwargs={\n",
      "                    'seed': 42,\n",
      "                },\n",
      "                openai_api_key=SecretStr('**********'),\n",
      "                openai_proxy='',\n",
      "                max_tokens=2048,\n",
      "            ),\n",
      "            'gpt_4o_openai': ChatOpenAI(\n",
      "                client=<openai.resources.chat.completions.Completions object at 0x7f750ff3d030>,\n",
      "                async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f750ff3ee00>,\n",
      "                model_name='gpt-4o',\n",
      "                temperature=0.0,\n",
      "                model_kwargs={\n",
      "                    'seed': 42,\n",
      "                },\n",
      "                openai_api_key=SecretStr('**********'),\n",
      "                openai_proxy='',\n",
      "                max_tokens=2048,\n",
      "            ),\n",
      "            'llama3_70_deepinfra': ChatDeepInfra(\n",
      "                name='meta-llama/Meta-Llama-3-70B-Instruct',\n",
      "                deepinfra_api_token='rRHmU56sHzgr0bvZYbPYhogfCWTezIZT',\n",
      "                model_kwargs={\n",
      "                    'temperature': 0,\n",
      "                    'max_new_tokens': 2048,\n",
      "                    'repetition_penalty': 1.3,\n",
      "                    'stop': [\n",
      "                        'STOP_TOKEN',\n",
      "                    ],\n",
      "                },\n",
      "            ),\n",
      "            'llama3_8_deepinfra': ChatDeepInfra(\n",
      "                name='meta-llama/Meta-Llama-3-8B-Instruct',\n",
      "                deepinfra_api_token='rRHmU56sHzgr0bvZYbPYhogfCWTezIZT',\n",
      "                model_kwargs={\n",
      "                    'temperature': 0,\n",
      "                    'max_new_tokens': 2048,\n",
      "                    'repetition_penalty': 1.3,\n",
      "                    'stop': [\n",
      "                        'STOP_TOKEN',\n",
      "                    ],\n",
      "                },\n",
      "            ),\n",
      "            'mixtral_7x8_deepinfra': ChatDeepInfra(\n",
      "                name='mistralai/Mixtral-8x7B-Instruct-v0.1',\n",
      "                deepinfra_api_token='rRHmU56sHzgr0bvZYbPYhogfCWTezIZT',\n",
      "                model_kwargs={\n",
      "                    'temperature': 0,\n",
      "                    'max_new_tokens': 2048,\n",
      "                    'repetition_penalty': 1.3,\n",
      "                    'stop': [\n",
      "                        'STOP_TOKEN',\n",
      "                    ],\n",
      "                },\n",
      "            ),\n",
      "            'llama3_70_deepinfra_lite': ChatLiteLLM(\n",
      "                client=(\n",
      "                    <module 'litellm' from '/home/tcl/.cache/pypoetry/virtualenvs/genai-\n",
      "                    training-2X6HL8i2-py3.10/lib/python3.10/site-packages/litellm/__init__.py'>\n",
      "                ),\n",
      "                model='deepinfra/meta-llama/Llama-3-70b-chat-hf',\n",
      "                openai_api_key='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',\n",
      "                azure_api_key='',\n",
      "                anthropic_api_key='',\n",
      "                replicate_api_key='',\n",
      "                cohere_api_key='',\n",
      "                openrouter_api_key='',\n",
      "                temperature=0.0,\n",
      "                huggingface_api_key='',\n",
      "                together_ai_api_key='',\n",
      "            ),\n",
      "            'llama3_70_groq': ChatGroq(\n",
      "                client=<groq.resources.chat.completions.Completions object at 0x7f750ff3fb50>,\n",
      "                async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7f750ff48850>,\n",
      "                model_name='lLama3-70b-8192',\n",
      "                temperature=1e-08,\n",
      "                groq_api_key=SecretStr('**********'),\n",
      "                max_tokens=2048,\n",
      "            ),\n",
      "            'llama3_8_groq': ChatGroq(\n",
      "                client=<groq.resources.chat.completions.Completions object at 0x7f750ff495a0>,\n",
      "                async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7f750ff4a260>,\n",
      "                model_name='lLama3-8b-8192',\n",
      "                temperature=1e-08,\n",
      "                groq_api_key=SecretStr('**********'),\n",
      "                max_tokens=2048,\n",
      "            ),\n",
      "            'mixtral_7x8_groq': ChatGroq(\n",
      "                client=<groq.resources.chat.completions.Completions object at 0x7f750ff4afb0>,\n",
      "                async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7f750ff4bc70>,\n",
      "                model_name='Mixtral-8x7b-32768',\n",
      "                temperature=1e-08,\n",
      "                groq_api_key=SecretStr('**********'),\n",
      "                max_tokens=2048,\n",
      "            ),\n",
      "            'llama3_8_local': ChatOllama(\n",
      "                model='llama3:instruct',\n",
      "                temperature=0.0,\n",
      "            ),\n",
      "            'gpt_4o_edenai': ChatEdenAI(\n",
      "                model='gpt-4o',\n",
      "                max_tokens=2048,\n",
      "                edenai_api_key=SecretStr('**********'),\n",
      "            ),\n",
      "            'gpt_4_edenai': ChatEdenAI(\n",
      "                model='gpt-4',\n",
      "                max_tokens=2048,\n",
      "                edenai_api_key=SecretStr('**********'),\n",
      "            ),\n",
      "            'mistral_large_edenai': ChatEdenAI(\n",
      "                provider='mistral',\n",
      "                model='large-latest',\n",
      "                max_tokens=2048,\n",
      "                edenai_api_key=SecretStr('**********'),\n",
      "            ),\n",
      "            'gpt_4_azure': AzureChatOpenAI(\n",
      "                name='gpt4-turbo',\n",
      "                client=<openai.resources.chat.completions.Completions object at 0x7f750ff59c30>,\n",
      "                async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f750ff5ba00>,\n",
      "                model_name='gpt4-turbo',\n",
      "                temperature=0.0,\n",
      "                openai_api_key=SecretStr('**********'),\n",
      "                openai_proxy='',\n",
      "                azure_endpoint='https://mutualizedopenai.openai.azure.com',\n",
      "                deployment_name='gpt4-turbo',\n",
      "                openai_api_version='2023-05-15',\n",
      "                openai_api_type='azure',\n",
      "            ),\n",
      "            'gpt_35_azure': AzureChatOpenAI(\n",
      "                name='gpt-35-turbo',\n",
      "                client=<openai.resources.chat.completions.Completions object at 0x7f750ff718a0>,\n",
      "                async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f750ff73640>,\n",
      "                model_name='gpt-35-turbo',\n",
      "                temperature=0.0,\n",
      "                openai_api_key=SecretStr('**********'),\n",
      "                openai_proxy='',\n",
      "                azure_endpoint='https://mutualizedopenai.openai.azure.com',\n",
      "                deployment_name='gpt-35-turbo',\n",
      "                openai_api_version='2023-05-15',\n",
      "                openai_api_type='azure',\n",
      "            ),\n",
      "            'gpt_4o_azure': AzureChatOpenAI(\n",
      "                name='gpt-4o',\n",
      "                client=<openai.resources.chat.completions.Completions object at 0x7f750ff854e0>,\n",
      "                async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f750ff872b0>,\n",
      "                model_name='gpt-4o',\n",
      "                temperature=0.0,\n",
      "                openai_api_key=SecretStr('**********'),\n",
      "                openai_proxy='',\n",
      "                azure_endpoint='https://mutualizedopenai.openai.azure.com',\n",
      "                deployment_name='gpt-4o',\n",
      "                openai_api_version='2023-05-15',\n",
      "                openai_api_type='azure',\n",
      "            ),\n",
      "        },\n",
      "        default_key='gpt_35_edenai',\n",
      "        prefix_keys=False,\n",
      "    ) (RunnableConfigurableAlternatives)\n"
     ]
    }
   ],
   "source": [
    "# Get the default LLM. We can configure the temperature, cache, max_token, ...\n",
    "llm_default = get_llm()\n",
    "debug(llm_default)\n",
    "\n",
    "# Note that the LLM is configurable, ie we can change the LLM at run time\n",
    "# See https://python.langchain.com/v0.1/docs/expression_language/primitives/configure/#with-llms-1\n",
    "# (not necessary for the training)\n",
    "\n",
    "# or get a given LLM;\n",
    "llama3_70 = LlmFactory(\n",
    "    llm_id=\"llama3_70_groq\"\n",
    ").get()  # Might NOT work if you din't have the API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send a prompt to the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_322719/3475873083.py:2 <module>\n",
      "    a: AIMessage(\n",
      "        content=(\n",
      "            \"Why couldn't the bicycle stand up by itself?\\n\"\n",
      "            '\\n'\n",
      "            'Because it was two tired!'\n",
      "        ),\n",
      "        response_metadata={\n",
      "            'openai': {\n",
      "                'status': 'success',\n",
      "                'generated_text': (\n",
      "                    \"Why couldn't the bicycle stand up by itself?\\n\"\n",
      "                    '\\n'\n",
      "                    'Because it was two tired!'\n",
      "                ),\n",
      "                'message': [\n",
      "                    {\n",
      "                        'role': 'user',\n",
      "                        'message': 'tell me a joke',\n",
      "                        'tools': None,\n",
      "                        'tool_calls': None,\n",
      "                    },\n",
      "                    {\n",
      "                        'role': 'assistant',\n",
      "                        'message': (\n",
      "                            \"Why couldn't the bicycle stand up by itself?\\n\"\n",
      "                            '\\n'\n",
      "                            'Because it was two tired!'\n",
      "                        ),\n",
      "                        'tools': None,\n",
      "                        'tool_calls': [],\n",
      "                    },\n",
      "                ],\n",
      "                'cost': 4.05e-05,\n",
      "            },\n",
      "        },\n",
      "        id='run-40697cf1-1c3f-4a65-b55e-318e5fc04938-0',\n",
      "    ) (AIMessage)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two tired!\", response_metadata={'openai': {'status': 'success', 'generated_text': \"Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two tired!\", 'message': [{'role': 'user', 'message': 'tell me a joke', 'tools': None, 'tool_calls': None}, {'role': 'assistant', 'message': \"Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two tired!\", 'tools': None, 'tool_calls': []}], 'cost': 4.05e-05}}, id='run-40697cf1-1c3f-4a65-b55e-318e5fc04938-0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = llm_default.invoke(\"tell me a joke\")\n",
    "debug(a)\n",
    "# Analyse the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call LLM ; Change dynamically.\n",
    "\n",
    "To be updated with available LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-27 00:03:43.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpython.ai_core.llm\u001b[0m:\u001b[36mget_llm\u001b[0m:\u001b[36m395\u001b[0m - \u001b[1mget LLM : gpt_35_edenai - configurable: True\u001b[0m\n",
      "\u001b[32m2024-06-27 00:03:43.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpython.ai_core.llm\u001b[0m:\u001b[36mget_configurable\u001b[0m:\u001b[36m351\u001b[0m - \u001b[1mCannot load gemini_pro_google: No module named 'langchain_google_vertexai'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_322719/1179150227.py:8 <module>\n",
      "    r1: AIMessage(\n",
      "        content=(\n",
      "            'Why did the bear go to the doctor?\\n'\n",
      "            '\\n'\n",
      "            'Because it had a grizzly cough!'\n",
      "        ),\n",
      "        response_metadata={\n",
      "            'token_usage': {\n",
      "                'completion_tokens': 18,\n",
      "                'prompt_tokens': 16,\n",
      "                'total_tokens': 34,\n",
      "                'completion_time': 0.051428571,\n",
      "                'prompt_time': 0.005283315,\n",
      "                'queue_time': None,\n",
      "                'total_time': 0.056711886,\n",
      "            },\n",
      "            'model_name': 'lLama3-70b-8192',\n",
      "            'system_fingerprint': 'fp_7ab5f7e105',\n",
      "            'finish_reason': 'stop',\n",
      "            'logprobs': None,\n",
      "        },\n",
      "        id='run-d51c867f-3692-4cb8-a64a-8e128f590a40-0',\n",
      "    ) (AIMessage)\n",
      "/tmp/ipykernel_322719/1179150227.py:11 <module>\n",
      "    r2: AIMessage(\n",
      "        content=(\n",
      "            'Why did the bear break up with his girlfriend? \\n'\n",
      "            '\\n'\n",
      "            \"Because he couldn't bear the relationship anymore!\"\n",
      "        ),\n",
      "        response_metadata={\n",
      "            'token_usage': {\n",
      "                'completion_tokens': 20,\n",
      "                'prompt_tokens': 13,\n",
      "                'total_tokens': 33,\n",
      "            },\n",
      "            'model_name': 'gpt-3.5-turbo-0125',\n",
      "            'system_fingerprint': None,\n",
      "            'finish_reason': 'stop',\n",
      "            'logprobs': None,\n",
      "        },\n",
      "        id='run-611f627b-2233-4884-946d-4b1e7db0ec2b-0',\n",
      "        usage_metadata={\n",
      "            'input_tokens': 13,\n",
      "            'output_tokens': 20,\n",
      "            'total_tokens': 33,\n",
      "        },\n",
      "    ) (AIMessage)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why did the bear break up with his girlfriend? \\n\\nBecause he couldn't bear the relationship anymore!\", response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 13, 'total_tokens': 33}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-611f627b-2233-4884-946d-4b1e7db0ec2b-0', usage_metadata={'input_tokens': 13, 'output_tokens': 20, 'total_tokens': 33})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm_dyn = get_llm()\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "chain = prompt | llm_dyn\n",
    "c1 = chain.with_config(configurable={\"llm\": \"llama3_70_groq\"})\n",
    "r1 = c1.invoke({\"topic\": \"bears\"})\n",
    "debug(r1)\n",
    "c2 = chain.with_config(configurable={\"llm\": \"gpt_35_openai\"})\n",
    "r2 = c2.invoke({\"topic\": \"bears\"})\n",
    "debug(r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-training-2X6HL8i2-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
