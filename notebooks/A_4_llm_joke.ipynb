{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Joke (First LLM calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from devtools import debug\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(verbose=True)\n",
    "\n",
    "!export PYTHONPATH=\":./python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Factory\n",
    "\n",
    "To facilitate the selection and configuration of LLM and their provider, we have an \"LLM Factory'. <br> <br>\n",
    "See [llm.py](../python/ai_core/llm.py)  <br>\n",
    "\n",
    "List of hard-coded LLM configuration is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt_35_openai',\n",
       " 'gpt_4o_openai',\n",
       " 'llama3_70_deepinfra',\n",
       " 'llama3_8_deepinfra',\n",
       " 'mixtral_7x8_deepinfra',\n",
       " 'llama3_70_deepinfra_lite',\n",
       " 'llama3_70_groq',\n",
       " 'llama3_8_groq',\n",
       " 'mixtral_7x8_groq',\n",
       " 'gemini_pro_google',\n",
       " 'llama3_8_local',\n",
       " 'gpt_4o_edenai',\n",
       " 'gpt_4_edenai',\n",
       " 'gpt_35_edenai',\n",
       " 'mistral_large_edenai',\n",
       " 'gpt_4_azure',\n",
       " 'gpt_35_azure',\n",
       " 'gpt_4o_azure']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from python.ai_core.llm import LlmFactory, get_llm\n",
    "\n",
    "LlmFactory().known_items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of an LLM and LLM provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'debug' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get the default LLM. We can configure the temperature, cache, max_token, ...\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m llm_default \u001b[38;5;241m=\u001b[39m \u001b[43mget_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m debug(llm_default)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Note that the LLM is configurable, ie we can change the LLM at run time\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# See https://python.langchain.com/v0.1/docs/expression_language/primitives/configure/#with-llms-1\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# (not necessary for the training)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# or get a given LLM;\u001b[39;00m\n",
      "File \u001b[0;32m~/prj/genai-blueprint/python/ai_core/llm.py:411\u001b[0m, in \u001b[0;36mget_llm\u001b[0;34m(llm_id, temperature, max_tokens, json_mode, cache, configurable, with_fallback)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03mCreate a BaseLanguageModel object according to a given llm_id.\\n\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m- If 'llm_id' is None, the LLM is selected from the configuration.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    402\u001b[0m \n\u001b[1;32m    403\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    404\u001b[0m factory \u001b[38;5;241m=\u001b[39m LlmFactory(\n\u001b[1;32m    405\u001b[0m     llm_id\u001b[38;5;241m=\u001b[39mllm_id,\n\u001b[1;32m    406\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    409\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m    410\u001b[0m )\n\u001b[0;32m--> 411\u001b[0m \u001b[43mdebug\u001b[49m(factory)\n\u001b[1;32m    412\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget LLM : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfactory\u001b[38;5;241m.\u001b[39mllm_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - configurable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfigurable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m configurable:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'debug' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the default LLM. We can configure the temperature, cache, max_token, ...\n",
    "\n",
    "\n",
    "llm_default = get_llm()\n",
    "debug(llm_default)\n",
    "\n",
    "# Note that the LLM is configurable, ie we can change the LLM at run time\n",
    "# See https://python.langchain.com/v0.1/docs/expression_language/primitives/configure/#with-llms-1\n",
    "# (not necessary for the training)\n",
    "\n",
    "# or get a given LLM;\n",
    "llama3_70 = LlmFactory(\n",
    "    llm_id=\"llama3_70_groq\"\n",
    ").get()  # Might NOT work if you din't have the API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send a prompt to the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = llm_default.invoke(\"tell me a joke\")\n",
    "debug(a)\n",
    "# Analyse the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call LLM ; Change dynamically.\n",
    "\n",
    "To be updated with available LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm_dyn = get_llm()\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "chain = prompt | llm_dyn\n",
    "c1 = chain.with_config(configurable={\"llm\": \"llama3_70_groq\"})\n",
    "r1 = c1.invoke({\"topic\": \"bears\"})\n",
    "debug(r1)\n",
    "c2 = chain.with_config(configurable={\"llm\": \"gpt_35_openai\"})\n",
    "r2 = c2.invoke({\"topic\": \"bears\"})\n",
    "debug(r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-training-2X6HL8i2-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
