{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Joke (First LLM calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Setup Cell.\n",
    "# It imports environment variables, define 'devtools.debug\" as a buildin, set PYTHONPATH and autorelaod\n",
    "# Copy it in other Notebooks\n",
    "\n",
    "import builtins\n",
    "\n",
    "from devtools import debug\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "builtins.debug = debug\n",
    "load_dotenv(verbose=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reset -f\n",
    "\n",
    "!export PYTHONPATH=\":./python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Factory\n",
    "\n",
    "To facilitate the selection and configuration of LLM and their provider, we have an \"LLM Factory'. <br> <br>\n",
    "See [llm.py](../python/ai_core/llm.py)  <br>\n",
    "\n",
    "List of hard-coded LLM configuration is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tcl/.cache/pypoetry/virtualenvs/genai-blueprint-2X6HL8i2-py3.12/lib/python3.12/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n",
      "\u001b[32m2025-01-30 17:30:18.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpython.config\u001b[0m:\u001b[36msingleton\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mload /home/tcl/prj/genai-blueprint/app_conf.yaml\u001b[0m\n",
      "\u001b[32m2025-01-30 17:30:18.673\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mpython.config\u001b[0m:\u001b[36msingleton\u001b[0m:\u001b[36m96\u001b[0m - \u001b[33m\u001b[1mConfiguration selected by environment variable 'BLUEPRINT_CONFIG' not found in config file: edc_local' \u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['claude_haiku35_openrouter',\n",
       " 'claude_sonnet35_openrouter',\n",
       " 'deepseek_R1_deepseek',\n",
       " 'deepseek_chatv3_deepseek',\n",
       " 'deepseek_chatv3_openrouter',\n",
       " 'fake_parrot_local',\n",
       " 'falcon3_3_ollama',\n",
       " 'gemma2_2_ollama',\n",
       " 'google_gemini15flash_edenai',\n",
       " 'google_gemini15flash_openrouter',\n",
       " 'google_gemini15pro_openrouter',\n",
       " 'gpt_35_azure',\n",
       " 'gpt_35_openai',\n",
       " 'gpt_4_edenai',\n",
       " 'gpt_4o_azure',\n",
       " 'gpt_4o_edenai',\n",
       " 'gpt_4o_openai',\n",
       " 'gpt_4omini_edenai',\n",
       " 'gpt_4omini_openai',\n",
       " 'gpt_4omini_openrouter',\n",
       " 'gpt_4t_azure',\n",
       " 'liquid_lfm40_openrouter',\n",
       " 'llama31_405_deepinfra',\n",
       " 'llama31_70_deepinfra',\n",
       " 'llama31_8_deepinfra',\n",
       " 'llama31_8_groq',\n",
       " 'llama32_3_ollama',\n",
       " 'llama32_90V_deepinfra',\n",
       " 'llama32_90V_openrouter',\n",
       " 'llama33_70_deepinfra',\n",
       " 'llama33_70_groq',\n",
       " 'llama33_70_openrouter',\n",
       " 'llava_phi3_ollama',\n",
       " 'mistral_large2_openrouter',\n",
       " 'mistral_large_edenai',\n",
       " 'mistral_nemo_openrouter',\n",
       " 'mixtral_7x8_deepinfra',\n",
       " 'mixtral_7x8_groq',\n",
       " 'nova_lite10_openrouter',\n",
       " 'nova_pro10_openrouter',\n",
       " 'nvidia_nemotrom70_deepinfra',\n",
       " 'nvidia_nemotrom70_openrouter',\n",
       " 'qwen25_72_openrouter',\n",
       " 'qwen25_coder32_openrouter',\n",
       " 'qwen2_70_deepinfra',\n",
       " 'qwen2_vl72_openrouter',\n",
       " 'sonar_reasoning_openrouter',\n",
       " 'wizard2_8x22_deepinfra']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from python.ai_core.llm import LlmFactory, get_llm\n",
    "\n",
    "LlmFactory().known_items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of an LLM and LLM provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-30 17:30:18.736\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpython.ai_core.llm\u001b[0m:\u001b[36mget_llm\u001b[0m:\u001b[36m499\u001b[0m - \u001b[1mget LLM:'gpt_4omini_openai'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_395710/123939480.py:5 <module>\n",
      "    llm_default: ChatOpenAI(\n",
      "        client=<openai.resources.chat.completions.Completions object at 0x7f1391348320>,\n",
      "        async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f1391349a30>,\n",
      "        root_client=<openai.OpenAI object at 0x7f13916ed520>,\n",
      "        root_async_client=<openai.AsyncOpenAI object at 0x7f1391348350>,\n",
      "        model_name='gpt-4o-mini',\n",
      "        temperature=0.0,\n",
      "        model_kwargs={},\n",
      "        openai_api_key=SecretStr('**********'),\n",
      "        openai_api_base='https://api.openai.com/v1/',\n",
      "        max_retries=2,\n",
      "        seed=42,\n",
      "    ) (ChatOpenAI)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unsupported LLM class AzureOpenAI",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m debug(llm_default)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# or get a given LLM;\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m gpt4 \u001b[38;5;241m=\u001b[39m \u001b[43mLlmFactory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt_4o_azure\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Might NOT work if you din't have the API key\u001b[39;00m\n",
      "File \u001b[0;32m~/prj/genai-blueprint/python/ai_core/llm.py:256\u001b[0m, in \u001b[0;36mLlmFactory.get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo known API key for : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 256\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m llm\n",
      "File \u001b[0;32m~/prj/genai-blueprint/python/ai_core/llm.py:403\u001b[0m, in \u001b[0;36mLlmFactory.model_factory\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo API key found for LLM: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsupported LLM class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m llm\n",
      "\u001b[0;31mValueError\u001b[0m: unsupported LLM class AzureOpenAI"
     ]
    }
   ],
   "source": [
    "# Get the default LLM. We can configure the temperature, cache, max_token, ...\n",
    "\n",
    "\n",
    "llm_default = get_llm()\n",
    "debug(llm_default)\n",
    "\n",
    "# or get a given LLM;\n",
    "gpt4 = LlmFactory(llm_id=\"gpt_4o_azure\").get()  # Might NOT work if you din't have the API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.ai_core.llm import get_configurable_llm, llm_config\n",
    "\n",
    "configurable_llm = get_configurable_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send a prompt to the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke1 = llm_default.invoke(\"tell me a sad joke\")\n",
    "debug(joke1)\n",
    "# Analyse the outcome\n",
    "\n",
    "joke2 = configurable_llm.with_config(llm_config(\"gpt_4omini_edenai\")).invoke(\"tell me a joke on data computers\")\n",
    "debug(joke2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call LLM ; Change dynamically.\n",
    "\n",
    "To be updated with available LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm_dyn = get_llm()\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "chain = prompt | llm_dyn\n",
    "c1 = chain.with_config(configurable={\"llm\": \"llama3_70_groq\"})\n",
    "r1 = c1.invoke({\"topic\": \"bears\"})\n",
    "debug(r1)\n",
    "c2 = chain.with_config(configurable={\"llm\": \"gpt_35_openai\"})\n",
    "r2 = c2.invoke({\"topic\": \"bears\"})\n",
    "debug(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_llm = get_llm(llm_type=\"fast_model\", llm_id=\"gpt_4omini_openai\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-blueprint-2X6HL8i2-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
