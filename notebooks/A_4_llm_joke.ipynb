{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Joke (First LLM calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Setup Cell.\n",
    "# It imports environment variables, define 'devtools.debug\" as a buildin, set PYTHONPATH and autorelaod\n",
    "# Copy it in other Notebooks\n",
    "\n",
    "import builtins\n",
    "\n",
    "from devtools import debug\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "builtins.debug = debug\n",
    "load_dotenv(verbose=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reset -f\n",
    "\n",
    "!export PYTHONPATH=\":./python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Factory\n",
    "\n",
    "To facilitate the selection and configuration of LLM and their provider, we have an \"LLM Factory'. <br> <br>\n",
    "See [llm.py](../python/ai_core/llm.py)  <br>\n",
    "\n",
    "List of hard-coded LLM configuration is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fake_parrot_local',\n",
       " 'falcon3_3_ollama',\n",
       " 'gemma2_2_ollama',\n",
       " 'google_gemini15flash_edenai',\n",
       " 'gpt_35_azure',\n",
       " 'gpt_35_openai',\n",
       " 'gpt_4_edenai',\n",
       " 'gpt_4o_azure',\n",
       " 'gpt_4o_edenai',\n",
       " 'gpt_4o_openai',\n",
       " 'gpt_4omini_edenai',\n",
       " 'gpt_4omini_openai',\n",
       " 'gpt_4t_azure',\n",
       " 'llama31_405_deepinfra',\n",
       " 'llama31_70_deepinfra',\n",
       " 'llama31_8_deepinfra',\n",
       " 'llama31_8_groq',\n",
       " 'llama32_3_ollama',\n",
       " 'llama32_90V_deepinfra',\n",
       " 'llama33_70_deepinfra',\n",
       " 'llama33_70_groq',\n",
       " 'llava_phi3_ollama',\n",
       " 'mistral_large_edenai',\n",
       " 'mixtral_7x8_deepinfra',\n",
       " 'mixtral_7x8_groq',\n",
       " 'nvidia_nemotrom70_deepinfra',\n",
       " 'qwen2_70_deepinfra',\n",
       " 'wizard2_8x22_deepinfra',\n",
       " 'zephyr_7b_huggingface']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.ai_core.llm import LlmFactory, get_llm\n",
    "\n",
    "LlmFactory.known_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LlmInfo(id='fake_parrot_local', provider='fake', model='parrot', key=''),\n",
       " LlmInfo(id='gpt_35_openai', provider='openai', model='gpt-3.5-turbo-0125', key='OPENAI_API_KEY'),\n",
       " LlmInfo(id='gpt_4o_openai', provider='openai', model='gpt-4o', key='OPENAI_API_KEY'),\n",
       " LlmInfo(id='gpt_4omini_openai', provider='openai', model='gpt-4o-mini', key='OPENAI_API_KEY'),\n",
       " LlmInfo(id='llama31_70_deepinfra', provider='huggingface', model='meta-llama/Meta-Llama-3.1-70B-Instruct', key='HUGGINGFACEHUB_API_TOKEN'),\n",
       " LlmInfo(id='llama33_70_deepinfra', provider='huggingface', model='meta-llama/Llama-3.3-70B-Instruct', key='HUGGINGFACEHUB_API_TOKEN'),\n",
       " LlmInfo(id='llama31_8_deepinfra', provider='huggingface', model='meta-llama/Meta-Llama-3.1-8B-Instruct', key='HUGGINGFACEHUB_API_TOKEN'),\n",
       " LlmInfo(id='mixtral_7x8_deepinfra', provider='huggingface', model='mistralai/Mixtral-8x7B-Instruct-v0.1', key='HUGGINGFACEHUB_API_TOKEN'),\n",
       " LlmInfo(id='qwen2_70_deepinfra', provider='huggingface', model='Qwen/Qwen2-72B-Instruct', key='HUGGINGFACEHUB_API_TOKEN'),\n",
       " LlmInfo(id='wizard2_8x22_deepinfra', provider='huggingface', model='microsoft/WizardLM-2-8x22B', key='HUGGINGFACEHUB_API_TOKEN'),\n",
       " LlmInfo(id='nvidia_nemotrom70_deepinfra', provider='huggingface', model='nvidia/Llama-3.1-Nemotron-70B-Instruct', key='HUGGINGFACEHUB_API_TOKEN'),\n",
       " LlmInfo(id='llama31_405_deepinfra', provider='huggingface', model='meta-llama/Meta-Llama-3.1-405B-Instruct', key='HUGGINGFACEHUB_API_TOKEN'),\n",
       " LlmInfo(id='llama32_90V_deepinfra', provider='huggingface', model='meta-llama/Llama-3.2-90B-Vision-Instruct', key='HUGGINGFACEHUB_API_TOKEN'),\n",
       " LlmInfo(id='llama33_70_groq', provider='groq', model='llama-3.3-70b-versatile', key='GROQ_API_KEY'),\n",
       " LlmInfo(id='llama31_8_groq', provider='groq', model='llama-3.1-8b-instant', key='GROQ_API_KEY'),\n",
       " LlmInfo(id='mixtral_7x8_groq', provider='groq', model='Mixtral-8x7b-32768', key='GROQ_API_KEY'),\n",
       " LlmInfo(id='llama31_70_together', provider='together', model='meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo', key='TOGETHER_API_KEY'),\n",
       " LlmInfo(id='qwen2_72_together', provider='together', model='Qwen/Qwen1.5-72B-Chat', key='TOGETHER_API_KEY'),\n",
       " LlmInfo(id='gemini_pro_google', provider='google_vertexai', model='gemini-pro', key='GOOGLE_API_KEY'),\n",
       " LlmInfo(id='llama32_3_ollama', provider='ollama', model='llama3.2:latest', key=''),\n",
       " LlmInfo(id='gemma2_2_ollama', provider='ollama', model='gemma2:2b', key=''),\n",
       " LlmInfo(id='llava_phi3_ollama', provider='ollama', model='llava-phi3', key=''),\n",
       " LlmInfo(id='falcon3_3_ollama', provider='ollama', model='falcon3', key=''),\n",
       " LlmInfo(id='gpt_4o_edenai', provider='edenai', model='openai/gpt-4o', key='EDENAI_API_KEY'),\n",
       " LlmInfo(id='gpt_4_edenai', provider='edenai', model='openai/gpt-4', key='EDENAI_API_KEY'),\n",
       " LlmInfo(id='gpt_4omini_edenai', provider='edenai', model='openai/gpt-4o-mini', key='EDENAI_API_KEY'),\n",
       " LlmInfo(id='mistral_large_edenai', provider='edenai', model='mistral/large-latest', key='EDENAI_API_KEY'),\n",
       " LlmInfo(id='google_gemini15flash_edenai', provider='edenai', model='google/gemini-1.5-flash', key='EDENAI_API_KEY'),\n",
       " LlmInfo(id='gpt_4t_azure', provider='azure_openai', model='gpt4-turbo/2023-05-15', key='AZURE_OPENAI_API_KEY=your_azure_openai_key_here'),\n",
       " LlmInfo(id='gpt_35_azure', provider='azure_openai', model='gpt-35-turbo/2023-05-15', key='AZURE_OPENAI_API_KEY=your_azure_openai_key_here'),\n",
       " LlmInfo(id='gpt_4o_azure', provider='azure_openai', model='gpt-4o/2024-02-15-preview', key='AZURE_OPENAI_API_KEY=your_azure_openai_key_here'),\n",
       " LlmInfo(id='deepseek_chatv3_deepseek', provider='deepseek', model='deepseek-chat', key='DEEPSEEK_API_KEY'),\n",
       " LlmInfo(id='deepseek_R1_deepseek', provider='deepseek', model='deepseek-reasoner', key='DEEPSEEK_API_KEY'),\n",
       " LlmInfo(id='zephyr_7b_huggingface', provider='huggingface', model='HuggingFaceH4/zephyr-7b-beta', key='HUGGINGFACEHUB_API_TOKEN'),\n",
       " LlmInfo(id='claude_sonnet35_openrouter', provider='anthropic', model='anthropic/claude-3.5-sonnet', key='ANTHROPIC_API_KEY'),\n",
       " LlmInfo(id='claude_haiku35_openrouter', provider='anthropic', model='anthropic/claude-3-5-haiku', key='ANTHROPIC_API_KEY')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LlmFactory.known_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of an LLM and LLM provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default LLM. We can configure the temperature, cache, max_token, ...\n",
    "\n",
    "\n",
    "llm_default = get_llm()\n",
    "debug(llm_default)\n",
    "\n",
    "# or get a given LLM;\n",
    "gpt4o = LlmFactory(llm_id=\"gpt_4o_azure\").get()  # Might NOT work if you din't have the API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ai_core.llm import get_configurable_llm\n",
    "\n",
    "configurable_llm = get_configurable_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send a prompt to the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke1 = gpt4o.invoke(\"tell me a sad joke\")\n",
    "debug(joke1)\n",
    "# Analyse the outcome\n",
    "\n",
    "# joke2 = configurable_llm.with_config(llm_config(\"gpt_4omini_edenai\")).invoke(\"tell me a joke on data computers\")\n",
    "# debug(joke2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call LLM ; Change dynamically.\n",
    "\n",
    "To be updated with available LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm_dyn = get_llm()\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "chain = prompt | llm_dyn\n",
    "c1 = chain.with_config(configurable={\"llm\": \"llama3_70_groq\"})\n",
    "r1 = c1.invoke({\"topic\": \"bears\"})\n",
    "debug(r1)\n",
    "c2 = chain.with_config(configurable={\"llm\": \"gpt_35_openai\"})\n",
    "r2 = c2.invoke({\"topic\": \"bears\"})\n",
    "debug(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_llm = get_llm(llm_type=\"fast_model\", llm_id=\"gpt_4o_azure\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
