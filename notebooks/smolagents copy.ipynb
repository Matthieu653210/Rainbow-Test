{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from devtools import debug\n",
    "import builtins\n",
    "\n",
    "setattr(builtins, \"debug\", debug)\n",
    "load_dotenv(verbose=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "load_dotenv(verbose=True)\n",
    "!export PYTHONPATH=\":./python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login  --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install smolagents pandas langchain langchain-community sentence-transformers faiss-cpu accelerate rank_bm25 --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install diffusers spaces gradio_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import Tool, CodeAgent, HfApiModel, DuckDuckGoSearchTool, ToolCallingAgent, PythonInterpreterTool, TOOL_CALLING_SYSTEM_PROMPT, ToolCollection\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.agents import load_tools\n",
    "# import accelerate\n",
    "# import requests\n",
    "# from gradio_client import Client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import tool, HfApiModel, TransformersModel, LiteLLMModel\n",
    "\n",
    "from python.ai_core.llm import LlmFactory\n",
    "\n",
    "\n",
    "MODEL_ID = \"gpt_4o_openai\"\n",
    "model_name = LlmFactory(llm_id=MODEL_ID).get_litellm_model_name()\n",
    "model = LiteLLMModel(model_id=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str, celsius: Optional[bool] = False) -> str:\n",
    "    \"\"\"\n",
    "    Get weather in the next days at given location.\n",
    "    Secretly this tool does not care about the location, it hates the weather everywhere.\n",
    "\n",
    "    Args:\n",
    "        location: the location\n",
    "        celsius: the temperature\n",
    "    \"\"\"\n",
    "    return \"The weather is UNGODLY with torrential rains and temperatures below -10°C\"\n",
    "\n",
    "agent = ToolCallingAgent(tools=[get_weather], model=model)\n",
    "\n",
    "print(agent.run(\"What's the weather like in Paris?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG System with `smolagents 🤗` and `langchain 🦜️🔗`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RetrieverTool\n",
    "class RetrieverTool(Tool):\n",
    "    name = \"retriever\"\n",
    "    description = \"Uses semantic search to retrieve relevant documentation\"\n",
    "    inputs = {\n",
    "        \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The query in affirmative form rather than a question\"\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def __init__(self, docs, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.retriever = BM25Retriever.from_documents(docs, k=10)\n",
    "\n",
    "    def forward(self, query: str) -> str:\n",
    "        docs = self.retriever.invoke(query)\n",
    "        return \"\\nRetrieved documents:\\n\" + \"\".join(\n",
    "            f\"\\n\\n===== Document {i} =====\\n{doc.page_content}\"\n",
    "            for i, doc in enumerate(docs)\n",
    "        )\n",
    "\n",
    "# Process documents\n",
    "def prepare_docs(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "# Initialize agent\n",
    "def create_rag_agent(processed_docs):\n",
    "    retriever_tool = RetrieverTool(processed_docs)\n",
    "    return CodeAgent(\n",
    "        tools=[retriever_tool],\n",
    "        model=HfApiModel(),\n",
    "        max_iterations=4,\n",
    "        verbose=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and combine datasets\n",
    "dataset = load_dataset(\"Svngoku/African-History-Extra-11-30-24\")\n",
    "train_docs = dataset[\"train\"]\n",
    "test_docs = dataset[\"test\"]\n",
    "\n",
    "# Combine the documents\n",
    "source_docs = concatenate_datasets([train_docs, test_docs])\n",
    "\n",
    "# Create documents using the content field and metadata\n",
    "source_docs = [\n",
    "    Document(\n",
    "        page_content=item['content'],  # Use the content field for the main text\n",
    "        metadata={\n",
    "            \"source\": item['url'],\n",
    "            \"title\": item['title'],\n",
    "            \"description\": item['description'],\n",
    "            \"published_time\": item['publishedTime']\n",
    "        }\n",
    "    )\n",
    "    for item in source_docs\n",
    "]\n",
    "\n",
    "# Process and initialize\n",
    "processed_docs = prepare_docs(source_docs)\n",
    "agent = create_rag_agent(processed_docs)\n",
    "\n",
    "# Run query\n",
    "response = agent.run(\"What is the main topic on the history of Meroe ?\")\n",
    "\n",
    "debug(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CodeAgent with `smolagents 🤗`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = CodeAgent(\n",
    "    tools=[\n",
    "        DuckDuckGoSearchTool()\n",
    "    ],\n",
    "    model=HfApiModel(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.run(\"What is the library smolagents ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = CodeAgent(\n",
    "    tools=[\n",
    "        DuckDuckGoSearchTool()\n",
    "    ],\n",
    "    model=HfApiModel(),\n",
    "    additional_authorized_imports=['requests', 'bs4', 'langchain', 'urlopen'],\n",
    "    max_iterations=4,\n",
    "    verbose=True\n",
    ")\n",
    "agent.run(\"Could you get me the summary of the page at url 'https://www.africanhistoryextra.com/p/on-the-nubian-priests-of-rome-and'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(agent.system_prompt_template))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToolCallingAgent with `smolagents 🤗`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_prompt = TOOL_CALLING_SYSTEM_PROMPT\n",
    "\n",
    "agent = ToolCallingAgent(tools=[PythonInterpreterTool()], model=HfApiModel(), system_prompt=modified_prompt)\n",
    "\n",
    "agent.run(\n",
    "    \"If The current world population is 8,196,550,521 as of December 30, 2024 and the population growth rate is 1.1%, what will the population be in 2050? Give the answer in billions using the compound growth formula\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(agent.system_prompt_template))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Tool with `smolagents 🤗`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFModelDownloadsTool(Tool):\n",
    "    name = \"model_download_counter\"\n",
    "    description = \"\"\"\n",
    "    This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub.\n",
    "    It returns the name of the checkpoint.\"\"\"\n",
    "    inputs = {\n",
    "        \"task\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"the task category (such as text-classification, depth-estimation, etc)\",\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def forward(self, task: str):\n",
    "        from huggingface_hub import list_models\n",
    "\n",
    "        model = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\n",
    "        return model.id\n",
    "\n",
    "model_downloads_tool = HFModelDownloadsTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stable Diffusion 3.5 : Image Generator Tool with `smolagents 🤗`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StableDiffusionTool(Tool):\n",
    "    name = \"image_generator\"\n",
    "    description = \"Generate an image using Stable Diffusion 3.5\"\n",
    "    inputs = {\n",
    "        \"prompt\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The text prompt to generate an image from\"\n",
    "        },\n",
    "        \"negative_prompt\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Things to avoid in the generated image\",\n",
    "            \"default\": \"\",\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"width\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Image width in pixels\",\n",
    "            \"default\": 1024,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"height\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Image height in pixels\",\n",
    "            \"default\": 1024,\n",
    "            \"nullable\": True\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.client = Client(\"Svngoku/stable-diffusion-3.5-large\")\n",
    "\n",
    "    def forward(self, prompt: str, negative_prompt: str = \"\", width: int = 1024, height: int = 1024) -> str:\n",
    "        result = self.client.predict(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            seed=0,\n",
    "            randomize_seed=True,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            guidance_scale=4.5,\n",
    "            num_inference_steps=40,\n",
    "            api_name=\"/infer\"\n",
    "        )\n",
    "        return result\n",
    "\n",
    "# Create and use the tool\n",
    "sd_tool = StableDiffusionTool()\n",
    "result = sd_tool(\"A beautiful sunset on a beach\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FluxMergedTool(Tool):\n",
    "    name = \"flux_merged\"\n",
    "    description = \"Generate an image using FLUX.1-merged model\"\n",
    "    inputs = {\n",
    "        \"prompt\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The text prompt to generate an image from\"\n",
    "        },\n",
    "        \"width\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Image width in pixels\",\n",
    "            \"default\": 1024,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"height\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Image height in pixels\",\n",
    "            \"default\": 1024,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"guidance_scale\": {\n",
    "            \"type\": \"number\",\n",
    "            \"description\": \"Guidance scale for generation\",\n",
    "            \"default\": 3.5,\n",
    "            \"nullable\": True\n",
    "        },\n",
    "        \"num_inference_steps\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Number of denoising steps\",\n",
    "            \"default\": 8,\n",
    "            \"nullable\": True\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"  # Path to generated image\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.client = Client(\"multimodalart/FLUX.1-merged\")\n",
    "\n",
    "    def forward(self, \n",
    "                prompt: str, \n",
    "                width: int = 1024, \n",
    "                height: int = 1024, \n",
    "                guidance_scale: float = 3.5,\n",
    "                num_inference_steps: int = 8) -> str:\n",
    "        result = self.client.predict(\n",
    "            prompt=prompt,\n",
    "            seed=0,\n",
    "            randomize_seed=True,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            api_name=\"/infer\"\n",
    "        )\n",
    "        return result\n",
    "\n",
    "# Create and test the tool\n",
    "flux_merged_tool = FluxMergedTool()\n",
    "result = flux_merged_tool(\"A magical forest with glowing mushrooms\")\n",
    "print(result)\n",
    "\n",
    "# Optional: Display the image if you're in a notebook\n",
    "from IPython.display import Image\n",
    "Image(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HfApiModel(\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "agent = CodeAgent(tools=[flux_merged_tool], model=model)\n",
    "\n",
    "agent.run(\n",
    "    \"Improve this prompt : A rabbit wearing a space suit, then generate an image of it.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"/private/var/folders/65/rsl7lrd91h178dgdgmq6ks9r0000gn/T/gradio/4d150956011e9cbdbaf1baeac3542797d0978fd08e03f777e7870c0c68411b96/image.webp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langchain Tool with `smolagents 🤗` Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_tool = Tool.from_langchain(load_tools([\"serpapi\"])[0])\n",
    "\n",
    "# agent = CodeAgent(tools=[search_tool], model=HfApiModel())\n",
    "\n",
    "# agent.run(\"How many more blocks (also denoted as layers) are in BERT base encoder compared to the encoder from the architecture proposed in Attention is All You Need?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CodeAgent toolbox with `smolagents 🤗`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = CodeAgent(tools=[], model=HfApiModel(), add_base_tools=True)    \n",
    "# agent.toolbox.add_tool(model_downloads_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langchain Tool Collection with `smolagents 🤗`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import ToolCollection, CodeAgent\n",
    "\n",
    "# image_tool_collection = ToolCollection(\n",
    "#     collection_slug=\"huggingface-tools/diffusion-tools-6630bb19a942c2306a2cdb6f\",\n",
    "#     token=\"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\",\n",
    "# )\n",
    "# agent = CodeAgent(tools=[*image_tool_collection.tools], model=HfApiModel(), add_base_tools=True)\n",
    "\n",
    "# agent.run(\"Please draw me a picture of rivers and lakes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secure code execution with `smolagents 🤗`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Agents with `smolagents 🤗`\n",
    "Multiple research papers have shown that having the LLM write its actions (the tool calls) in code is much better than the current standard format for tool calling, which is across the industry different shades of “writing actions as a JSON of tools names and arguments to use”.\n",
    "\n",
    "Why is code better? Well, because we crafted our code languages specifically to be great at expressing actions performed by a computer. If JSON snippets was a better way, this package would have been written in JSON snippets and the devil would be laughing at us.\n",
    "\n",
    "Code is just a better way to express actions on a computer. It has better:\n",
    "\n",
    "- `Composability`: could you nest JSON actions within each other, or define a set of JSON actions to re-use later, the same way you could just define a python function?\n",
    "  \n",
    "- `Object management`: how do you store the output of an action like generate_image in JSON?\n",
    "  \n",
    "- `Generality`: code is built to express simply anything you can do have a computer do.\n",
    "\n",
    "- `Representation in LLM training corpuses`: why not leverage this benediction of the sky that plenty of quality actions have already been included in LLM training corpuses?\n",
    "\n",
    "This is illustrated on the figure below, taken from Executable Code Actions Elicit Better LLM Agents.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/code_vs_json_actions.png\" alt=\"alt text\" width=\"800\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env E2B_API_KEY=e2b_d41c35bf6016cd037987b00703e2ddefa5f66dbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import CodeAgent, VisitWebpageTool, HfApiModel\n",
    "\n",
    "agent = CodeAgent(\n",
    "    tools = [VisitWebpageTool()],\n",
    "    model=HfApiModel(),\n",
    "    additional_authorized_imports=[\"requests\", \"markdownify\", \"bs4\"],\n",
    "    use_e2b_executor=True,\n",
    ")\n",
    "\n",
    "agent.run(\"Qui est l'actuel premier ministre en France ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Correcting Text to SQL with `smolagents 🤗`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    MetaData,\n",
    "    Table,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    "    Float,\n",
    "    insert,\n",
    "    inspect,\n",
    "    text,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"sqlite:///:memory:\")\n",
    "metadata_obj = MetaData()\n",
    "\n",
    "\n",
    "table_name = \"receipts\"\n",
    "receipts = Table(\n",
    "    table_name,\n",
    "    metadata_obj,\n",
    "    Column(\"receipt_id\", Integer, primary_key=True),\n",
    "    Column(\"customer_name\", String(16), primary_key=True),\n",
    "    Column(\"price\", Float),\n",
    "    Column(\"tip\", Float),\n",
    ")\n",
    "metadata_obj.create_all(engine)\n",
    "\n",
    "rows = [\n",
    "    {\"receipt_id\": 1, \"customer_name\": \"Alan Payne\", \"price\": 12.06, \"tip\": 1.20},\n",
    "    {\"receipt_id\": 2, \"customer_name\": \"Alex Mason\", \"price\": 23.86, \"tip\": 0.24},\n",
    "    {\"receipt_id\": 3, \"customer_name\": \"Woodrow Wilson\", \"price\": 53.43, \"tip\": 5.43},\n",
    "    {\"receipt_id\": 4, \"customer_name\": \"Margaret James\", \"price\": 21.11, \"tip\": 1.00},\n",
    "    {\"receipt_id\": 5, \"customer_name\": \"John Doe\", \"price\": 100.00, \"tip\": 10.00},\n",
    "]\n",
    "for row in rows:\n",
    "    stmt = insert(receipts).values(**row)\n",
    "    with engine.begin() as connection:\n",
    "        cursor = connection.execute(stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspector = inspect(engine)\n",
    "columns_info = [(col[\"name\"], col[\"type\"]) for col in inspector.get_columns(\"receipts\")]\n",
    "\n",
    "table_description = \"Columns:\\n\" + \"\\n\".join([f\"  - {name}: {col_type}\" for name, col_type in columns_info])\n",
    "print(table_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import tool\n",
    "\n",
    "@tool\n",
    "def sql_engine(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Allows you to perform SQL queries on the table. Returns a string representation of the result.\n",
    "    The table is named 'receipts'. Its description is as follows:\n",
    "        Columns:\n",
    "        - receipt_id: INTEGER\n",
    "        - customer_name: VARCHAR(16)\n",
    "        - price: FLOAT\n",
    "        - tip: FLOAT\n",
    "\n",
    "    Args:\n",
    "        query: The query to perform. This should be correct SQL.\n",
    "    \"\"\"\n",
    "    output = \"\"\n",
    "    with engine.connect() as con:\n",
    "        rows = con.execute(text(query))\n",
    "        for row in rows:\n",
    "            output += \"\\n\" + str(row)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = CodeAgent(\n",
    "    tools=[sql_engine],\n",
    "    model=HfApiModel(\"Qwen/Qwen2.5-Coder-32B-Instruct\"),\n",
    "    verbose=True\n",
    ")\n",
    "agent.run(\"Can you give me the name of the client who got the most expensive receipt?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"waiters\"\n",
    "receipts = Table(\n",
    "    table_name,\n",
    "    metadata_obj,\n",
    "    Column(\"receipt_id\", Integer, primary_key=True),\n",
    "    Column(\"waiter_name\", String(16), primary_key=True),\n",
    ")\n",
    "metadata_obj.create_all(engine)\n",
    "\n",
    "rows = [\n",
    "    {\"receipt_id\": 1, \"waiter_name\": \"Corey Johnson\"},\n",
    "    {\"receipt_id\": 2, \"waiter_name\": \"Michael Watts\"},\n",
    "    {\"receipt_id\": 3, \"waiter_name\": \"Michael Watts\"},\n",
    "    {\"receipt_id\": 4, \"waiter_name\": \"Margaret James\"},\n",
    "]\n",
    "for row in rows:\n",
    "    stmt = insert(receipts).values(**row)\n",
    "    with engine.begin() as connection:\n",
    "        cursor = connection.execute(stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_description = \"\"\"Allows you to perform SQL queries on the table. Beware that this tool's output is a string representation of the execution output.\n",
    "It can use the following tables:\"\"\"\n",
    "\n",
    "inspector = inspect(engine)\n",
    "for table in [\"receipts\", \"waiters\"]:\n",
    "    columns_info = [(col[\"name\"], col[\"type\"]) for col in inspector.get_columns(table)]\n",
    "\n",
    "    table_description = f\"Table '{table}':\\n\"\n",
    "\n",
    "    table_description += \"Columns:\\n\" + \"\\n\".join([f\"  - {name}: {col_type}\" for name, col_type in columns_info])\n",
    "    updated_description += \"\\n\\n\" + table_description\n",
    "\n",
    "print(updated_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_engine.description = updated_description\n",
    "\n",
    "agent = CodeAgent(\n",
    "    tools=[sql_engine],\n",
    "    model=HfApiModel(\"Qwen/Qwen2.5-Coder-32B-Instruct\"),\n",
    ")\n",
    "\n",
    "agent.run(\"Which waiter got more total money from tips ? give only the name of the waiter\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-blueprint-2X6HL8i2-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
