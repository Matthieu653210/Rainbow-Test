{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from devtools import debug\n",
    "\n",
    "[sys.path.append(str(path)) for path in [Path.cwd(), Path.cwd().parent, Path.cwd().parent/\"python\"] if str(path) not in sys.path]\n",
    "\n",
    "from python.ai_core.llm import LlmFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f58f7e456c0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f58f7e1cbe0>, model_name='gpt-3.5-turbo-0125', temperature=0.0, model_kwargs={'seed': 42}, openai_api_key='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', openai_proxy='', max_tokens=2048)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "llm_default = LlmFactory().get()\n",
    "llm_default \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tcl/.cache/pypoetry/virtualenvs/genai-training-GvhVNXsu-py3.10/lib/python3.10/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/tcl/.cache/pypoetry/virtualenvs/genai-training-GvhVNXsu-py3.10/lib/python3.10/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_info\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatLiteLLM(client=<module 'litellm' from '/home/tcl/.cache/pypoetry/virtualenvs/genai-training-GvhVNXsu-py3.10/lib/python3.10/site-packages/litellm/__init__.py'>, model='groq/llama3-70b-8192', openai_api_key='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', azure_api_key='', anthropic_api_key='', replicate_api_key='', cohere_api_key='', openrouter_api_key='', huggingface_api_key='', together_ai_api_key='')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama3 = LlmFactory(llm_id=\"llama3_70_groq\").get()\n",
    "llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here's one:\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\n(wait for it...)\\n\\nBecause it was two-tired!\\n\\nHope that made you smile!\", response_metadata={'token_usage': Usage(completion_tokens=31, prompt_tokens=14, total_tokens=45), 'model': 'groq/llama3-70b-8192', 'finish_reason': 'stop'}, id='run-9a2c5167-b40b-4912-b070-e9e7b2f9579c-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama3.invoke(\"tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_541021/4116461010.py:10 <module>\n",
      "    r1: AIMessage(\n",
      "        content=(\n",
      "            'Why did the bear break up with his girlfriend? \\n'\n",
      "            '\\n'\n",
      "            \"Because he couldn't bear the relationship any longer!\"\n",
      "        ),\n",
      "        response_metadata={\n",
      "            'token_usage': {\n",
      "                'completion_tokens': 21,\n",
      "                'prompt_tokens': 13,\n",
      "                'total_tokens': 34,\n",
      "            },\n",
      "            'model_name': 'gpt-3.5-turbo-0125',\n",
      "            'system_fingerprint': 'fp_3b956da36b',\n",
      "            'finish_reason': 'stop',\n",
      "            'logprobs': None,\n",
      "        },\n",
      "        id='run-1ddc3bdc-211e-420e-8a7c-364deeaa1246-0',\n",
      "    ) (AIMessage)\n",
      "    r2: AIMessage(\n",
      "        content=(\n",
      "            'Why did the bear go to the doctor?\\n'\n",
      "            '\\n'\n",
      "            'Because it had a grizzly cough!'\n",
      "        ),\n",
      "        response_metadata={\n",
      "            'token_usage': Usage(\n",
      "                completion_tokens=17,\n",
      "                prompt_tokens=16,\n",
      "                total_tokens=33,\n",
      "            ),\n",
      "            'model': 'groq/llama3-70b-8192',\n",
      "            'finish_reason': 'stop',\n",
      "        },\n",
      "        id='run-47ea1587-5246-4b63-b226-da747f035593-0',\n",
      "    ) (AIMessage)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(AIMessage(content=\"Why did the bear break up with his girlfriend? \\n\\nBecause he couldn't bear the relationship any longer!\", response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 13, 'total_tokens': 34}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None}, id='run-1ddc3bdc-211e-420e-8a7c-364deeaa1246-0'),\n",
       " AIMessage(content='Why did the bear go to the doctor?\\n\\nBecause it had a grizzly cough!', response_metadata={'token_usage': Usage(completion_tokens=17, prompt_tokens=16, total_tokens=33), 'model': 'groq/llama3-70b-8192', 'finish_reason': 'stop'}, id='run-47ea1587-5246-4b63-b226-da747f035593-0'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm_dyn = LlmFactory().get_configurable()\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "chain = prompt | llm_dyn\n",
    "\n",
    "r1 = chain.invoke({\"topic\": \"bears\"})\n",
    "r2 =  chain.with_config(configurable={\"llm\": \"llama3_70_groq\"}).invoke({\"topic\": \"bears\"})\n",
    "debug(r1,r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "di-maintenance-agent-demo-DCv9_gKb-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
