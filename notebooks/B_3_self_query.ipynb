{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from devtools import debug\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!export PYTHONPATH=\":./python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.ai_core.llm import get_llm\n",
    "from python.ai_core.prompts import def_prompt\n",
    "\n",
    "from python.ai_chains.B_2_self_query import get_query_constructor, get_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-28 00:14:43.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpython.ai_core.llm\u001b[0m:\u001b[36mget_llm\u001b[0m:\u001b[36m395\u001b[0m - \u001b[1mget LLM : gpt_35_edenai - configurable: True\u001b[0m\n",
      "\u001b[32m2024-06-28 00:14:44.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpython.ai_core.llm\u001b[0m:\u001b[36mget_configurable\u001b[0m:\u001b[36m351\u001b[0m - \u001b[1mCannot load gemini_pro_google: No module named 'langchain_google_vertexai'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_162455/997951588.py:6 <module>\n",
      "    r: RunnableSequence(\n",
      "        first=FewShotPromptTemplate(\n",
      "            input_variables=['query'],\n",
      "            examples=[\n",
      "                {\n",
      "                    'i': 1,\n",
      "                    'data_source': (\n",
      "                        '```json\\n'\n",
      "                        '{{\\n'\n",
      "                        '    \"content\": \"Lyrics of a song\",\\n'\n",
      "                        '    \"attributes\": {{\\n'\n",
      "                        '        \"artist\": {{\\n'\n",
      "                        '            \"type\": \"string\",\\n'\n",
      "                        '            \"description\": \"Name of the song artist\"\\n'\n",
      "                        '        }},\\n'\n",
      "                        '        \"length\": {{\\n'\n",
      "                        '            \"type\": \"integer\",\\n'\n",
      "                        '            \"description\": \"Length of the song in seconds\"\\n'\n",
      "                        '        }},\\n'\n",
      "                        '        \"genre\": {{\\n'\n",
      "                        '            \"type\": \"string\",\\n'\n",
      "                        '            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"\\n'\n",
      "                        '        }}\\n'\n",
      "                        '    }}\\n'\n",
      "                        '}}\\n'\n",
      "                        '```'\n",
      "                    ),\n",
      "                    'user_query': (\n",
      "                        'What are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in th'\n",
      "                        'e dance pop genre'\n",
      "                    ),\n",
      "                    'structured_request': (\n",
      "                        '```json\\n'\n",
      "                        '{{\\n'\n",
      "                        '    \"query\": \"teenager love\",\\n'\n",
      "                        '    \"filter\": \"and(or(eq(\\\\\"artist\\\\\", \\\\\"Taylor Swift\\\\\"), eq(\\\\\"artist\\\\\", \\\\\"Katy Perry\\\\\")), lt(\\\\'\n",
      "                        '\"length\\\\\", 180), eq(\\\\\"genre\\\\\", \\\\\"pop\\\\\"))\"\\n'\n",
      "                        '}}\\n'\n",
      "                        '```'\n",
      "                    ),\n",
      "                },\n",
      "                {\n",
      "                    'i': 2,\n",
      "                    'data_source': (\n",
      "                        '```json\\n'\n",
      "                        '{{\\n'\n",
      "                        '    \"content\": \"Lyrics of a song\",\\n'\n",
      "                        '    \"attributes\": {{\\n'\n",
      "                        '        \"artist\": {{\\n'\n",
      "                        '            \"type\": \"string\",\\n'\n",
      "                        '            \"description\": \"Name of the song artist\"\\n'\n",
      "                        '        }},\\n'\n",
      "                        '        \"length\": {{\\n'\n",
      "                        '            \"type\": \"integer\",\\n'\n",
      "                        '            \"description\": \"Length of the song in seconds\"\\n'\n",
      "                        '        }},\\n'\n",
      "                        '        \"genre\": {{\\n'\n",
      "                        '            \"type\": \"string\",\\n'\n",
      "                        '            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"\\n'\n",
      "                        '        }}\\n'\n",
      "                        '    }}\\n'\n",
      "                        '}}\\n'\n",
      "                        '```'\n",
      "                    ),\n",
      "                    'user_query': 'What are songs that were not published on Spotify',\n",
      "                    'structured_request': (\n",
      "                        '```json\\n'\n",
      "                        '{{\\n'\n",
      "                        '    \"query\": \"\",\\n'\n",
      "                        '    \"filter\": \"NO_FILTER\"\\n'\n",
      "                        '}}\\n'\n",
      "                        '```'\n",
      "                    ),\n",
      "                },\n",
      "            ],\n",
      "            example_prompt=PromptTemplate(\n",
      "                input_variables=[\n",
      "                    'data_source',\n",
      "                    'i',\n",
      "                    'structured_request',\n",
      "                    'user_query',\n",
      "                ],\n",
      "                template=(\n",
      "                    '<< Example {i}. >>\\n'\n",
      "                    'Data Source:\\n'\n",
      "                    '{data_source}\\n'\n",
      "                    '\\n'\n",
      "                    'User Query:\\n'\n",
      "                    '{user_query}\\n'\n",
      "                    '\\n'\n",
      "                    'Structured Request:\\n'\n",
      "                    '{structured_request}\\n'\n",
      "                ),\n",
      "            ),\n",
      "            suffix=(\n",
      "                '<< Example 3. >>\\n'\n",
      "                'Data Source:\\n'\n",
      "                '```json\\n'\n",
      "                '{{\\n'\n",
      "                '    \"content\": \"Brief summary of a movie\",\\n'\n",
      "                '    \"attributes\": {{\\n'\n",
      "                '    \"genre\": {{\\n'\n",
      "                '        \"description\": \"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thrill'\n",
      "                'er\\', \\'romance\\', \\'action\\', \\'animated\\']\",\\n'\n",
      "                '        \"type\": \"string\"\\n'\n",
      "                '    }},\\n'\n",
      "                '    \"year\": {{\\n'\n",
      "                '        \"description\": \"The year the movie was released\",\\n'\n",
      "                '        \"type\": \"integer\"\\n'\n",
      "                '    }},\\n'\n",
      "                '    \"director\": {{\\n'\n",
      "                '        \"description\": \"The name of the movie director\",\\n'\n",
      "                '        \"type\": \"string\"\\n'\n",
      "                '    }},\\n'\n",
      "                '    \"rating\": {{\\n'\n",
      "                '        \"description\": \"A 1-10 rating for the movie\",\\n'\n",
      "                '        \"type\": \"float\"\\n'\n",
      "                '    }}\\n'\n",
      "                '}}\\n'\n",
      "                '}}\\n'\n",
      "                '```\\n'\n",
      "                '\\n'\n",
      "                'User Query:\\n'\n",
      "                '{query}\\n'\n",
      "                '\\n'\n",
      "                'Structured Request:\\n'\n",
      "            ),\n",
      "            prefix=(\n",
      "                \"Your goal is to structure the user's query to match the request schema provided below.\\n\"\n",
      "                '\\n'\n",
      "                '<< Structured Request Schema >>\\n'\n",
      "                'When responding use a markdown code snippet with a JSON object formatted in the following schema:\\n'\n",
      "                '\\n'\n",
      "                '```json\\n'\n",
      "                '{{\\n'\n",
      "                '    \"query\": string \\\\ text string to compare to document contents\\n'\n",
      "                '    \"filter\": string \\\\ logical condition statement for filtering documents\\n'\n",
      "                '}}\\n'\n",
      "                '```\\n'\n",
      "                '\\n'\n",
      "                'The query string should contain only text that is expected to match the contents of documents. Any co'\n",
      "                'nditions in the filter should not be mentioned in the query as well.\\n'\n",
      "                '\\n'\n",
      "                'A logical condition statement is composed of one or more comparison and logical operation statements.'\n",
      "                '\\n'\n",
      "                '\\n'\n",
      "                'A comparison statement takes the form: `comp(attr, val)`:\\n'\n",
      "                '- `comp` (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparator\\n'\n",
      "                '- `attr` (string):  name of attribute to apply the comparison to\\n'\n",
      "                '- `val` (string): is the comparison value\\n'\n",
      "                '\\n'\n",
      "                'A logical operation statement takes the form `op(statement1, statement2, ...)`:\\n'\n",
      "                '- `op` (and | or | not): logical operator\\n'\n",
      "                '- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or mor'\n",
      "                'e statements to apply the operation to\\n'\n",
      "                '\\n'\n",
      "                'Make sure that you only use the comparators and logical operators listed above and no others.\\n'\n",
      "                'Make sure that filters only refer to attributes that exist in the data source.\\n'\n",
      "                'Make sure that filters only use the attributed names with its function names if there are functions a'\n",
      "                'pplied on them.\\n'\n",
      "                'Make sure that filters only use format `YYYY-MM-DD` when handling date data typed values.\\n'\n",
      "                'Make sure that filters take into account the descriptions of attributes and only make comparisons tha'\n",
      "                't are feasible given the type of data being stored.\\n'\n",
      "                'Make sure that filters are only used as needed. If there are no filters that should be applied return'\n",
      "                ' \"NO_FILTER\" for the filter value.'\n",
      "            ),\n",
      "        ),\n",
      "        middle=[\n",
      "            RunnableConfigurableAlternatives(\n",
      "                default=ChatEdenAI(\n",
      "                    model='gpt-3.5-turbo-0125',\n",
      "                    max_tokens=2048,\n",
      "                    edenai_api_key=SecretStr('**********'),\n",
      "                ),\n",
      "                which=ConfigurableField(\n",
      "                    id='llm',\n",
      "                    name=None,\n",
      "                    description=None,\n",
      "                    annotation=None,\n",
      "                    is_shared=False,\n",
      "                ),\n",
      "                alternatives={\n",
      "                    'gpt_35_openai': ChatOpenAI(\n",
      "                        client=<openai.resources.chat.completions.Completions object at 0x7f76ce2770d0>,\n",
      "                        async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f76ce2773d0>,\n",
      "                        model_name='gpt-3.5-turbo-0125',\n",
      "                        temperature=0.0,\n",
      "                        model_kwargs={\n",
      "                            'seed': 42,\n",
      "                        },\n",
      "                        openai_api_key=SecretStr('**********'),\n",
      "                        openai_proxy='',\n",
      "                        max_tokens=2048,\n",
      "                    ),\n",
      "                    'gpt_4o_openai': ChatOpenAI(\n",
      "                        client=<openai.resources.chat.completions.Completions object at 0x7f76cdf04fa0>,\n",
      "                        async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f76ce29c190>,\n",
      "                        model_name='gpt-4o',\n",
      "                        temperature=0.0,\n",
      "                        model_kwargs={\n",
      "                            'seed': 42,\n",
      "                        },\n",
      "                        openai_api_key=SecretStr('**********'),\n",
      "                        openai_proxy='',\n",
      "                        max_tokens=2048,\n",
      "                    ),\n",
      "                    'llama3_70_deepinfra': ChatDeepInfra(\n",
      "                        name='meta-llama/Meta-Llama-3-70B-Instruct',\n",
      "                        deepinfra_api_token='rRHmU56sHzgr0bvZYbPYhogfCWTezIZT',\n",
      "                        model_kwargs={\n",
      "                            'temperature': 0,\n",
      "                            'max_new_tokens': 2048,\n",
      "                            'repetition_penalty': 1.3,\n",
      "                            'stop': [\n",
      "                                'STOP_TOKEN',\n",
      "                            ],\n",
      "                        },\n",
      "                    ),\n",
      "                    'llama3_8_deepinfra': ChatDeepInfra(\n",
      "                        name='meta-llama/Meta-Llama-3-8B-Instruct',\n",
      "                        deepinfra_api_token='rRHmU56sHzgr0bvZYbPYhogfCWTezIZT',\n",
      "                        model_kwargs={\n",
      "                            'temperature': 0,\n",
      "                            'max_new_tokens': 2048,\n",
      "                            'repetition_penalty': 1.3,\n",
      "                            'stop': [\n",
      "                                'STOP_TOKEN',\n",
      "                            ],\n",
      "                        },\n",
      "                    ),\n",
      "                    'mixtral_7x8_deepinfra': ChatDeepInfra(\n",
      "                        name='mistralai/Mixtral-8x7B-Instruct-v0.1',\n",
      "                        deepinfra_api_token='rRHmU56sHzgr0bvZYbPYhogfCWTezIZT',\n",
      "                        model_kwargs={\n",
      "                            'temperature': 0,\n",
      "                            'max_new_tokens': 2048,\n",
      "                            'repetition_penalty': 1.3,\n",
      "                            'stop': [\n",
      "                                'STOP_TOKEN',\n",
      "                            ],\n",
      "                        },\n",
      "                    ),\n",
      "                    'llama3_70_deepinfra_lite': ChatLiteLLM(\n",
      "                        client=(\n",
      "                            <module 'litellm' from '/home/tcl/.cache/pypoetry/virtualenvs/genai-\n",
      "                            training-2X6HL8i2-py3.10/lib/python3.10/site-packages/litellm/__init__.py'>\n",
      "                        ),\n",
      "                        model='deepinfra/meta-llama/Llama-3-70b-chat-hf',\n",
      "                        openai_api_key='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',\n",
      "                        azure_api_key='',\n",
      "                        anthropic_api_key='',\n",
      "                        replicate_api_key='',\n",
      "                        cohere_api_key='',\n",
      "                        openrouter_api_key='',\n",
      "                        temperature=0.0,\n",
      "                        huggingface_api_key='',\n",
      "                        together_ai_api_key='',\n",
      "                    ),\n",
      "                    'llama3_70_groq': ChatGroq(\n",
      "                        client=<groq.resources.chat.completions.Completions object at 0x7f76cdf102e0>,\n",
      "                        async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7f76cdf11360>,\n",
      "                        model_name='lLama3-70b-8192',\n",
      "                        temperature=1e-08,\n",
      "                        groq_api_key=SecretStr('**********'),\n",
      "                        max_tokens=2048,\n",
      "                    ),\n",
      "                    'llama3_8_groq': ChatGroq(\n",
      "                        client=<groq.resources.chat.completions.Completions object at 0x7f76cdf13a30>,\n",
      "                        async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7f76ce3996f0>,\n",
      "                        model_name='lLama3-8b-8192',\n",
      "                        temperature=1e-08,\n",
      "                        groq_api_key=SecretStr('**********'),\n",
      "                        max_tokens=2048,\n",
      "                    ),\n",
      "                    'mixtral_7x8_groq': ChatGroq(\n",
      "                        client=<groq.resources.chat.completions.Completions object at 0x7f76ce398640>,\n",
      "                        async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7f76ce39b910>,\n",
      "                        model_name='Mixtral-8x7b-32768',\n",
      "                        temperature=1e-08,\n",
      "                        groq_api_key=SecretStr('**********'),\n",
      "                        max_tokens=2048,\n",
      "                    ),\n",
      "                    'llama3_8_local': ChatOllama(\n",
      "                        model='llama3:instruct',\n",
      "                        temperature=0.0,\n",
      "                    ),\n",
      "                    'gpt_4o_edenai': ChatEdenAI(\n",
      "                        model='gpt-4o',\n",
      "                        max_tokens=2048,\n",
      "                        edenai_api_key=SecretStr('**********'),\n",
      "                    ),\n",
      "                    'gpt_4_edenai': ChatEdenAI(\n",
      "                        model='gpt-4',\n",
      "                        max_tokens=2048,\n",
      "                        edenai_api_key=SecretStr('**********'),\n",
      "                    ),\n",
      "                    'mistral_large_edenai': ChatEdenAI(\n",
      "                        provider='mistral',\n",
      "                        model='large-latest',\n",
      "                        max_tokens=2048,\n",
      "                        edenai_api_key=SecretStr('**********'),\n",
      "                    ),\n",
      "                    'gpt_4_azure': AzureChatOpenAI(\n",
      "                        name='gpt4-turbo',\n",
      "                        client=<openai.resources.chat.completions.Completions object at 0x7f76ce39a080>,\n",
      "                        async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f76ce397580>,\n",
      "                        model_name='gpt4-turbo',\n",
      "                        temperature=0.0,\n",
      "                        openai_api_key=SecretStr('**********'),\n",
      "                        openai_proxy='',\n",
      "                        azure_endpoint='https://mutualizedopenai.openai.azure.com',\n",
      "                        deployment_name='gpt4-turbo',\n",
      "                        openai_api_version='2023-05-15',\n",
      "                        openai_api_type='azure',\n",
      "                    ),\n",
      "                    'gpt_35_azure': AzureChatOpenAI(\n",
      "                        name='gpt-35-turbo',\n",
      "                        client=<openai.resources.chat.completions.Completions object at 0x7f76ce38a140>,\n",
      "                        async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f76ce437460>,\n",
      "                        model_name='gpt-35-turbo',\n",
      "                        temperature=0.0,\n",
      "                        openai_api_key=SecretStr('**********'),\n",
      "                        openai_proxy='',\n",
      "                        azure_endpoint='https://mutualizedopenai.openai.azure.com',\n",
      "                        deployment_name='gpt-35-turbo',\n",
      "                        openai_api_version='2023-05-15',\n",
      "                        openai_api_type='azure',\n",
      "                    ),\n",
      "                    'gpt_4o_azure': AzureChatOpenAI(\n",
      "                        name='gpt-4o',\n",
      "                        client=<openai.resources.chat.completions.Completions object at 0x7f76cdf27370>,\n",
      "                        async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f76cdf25ea0>,\n",
      "                        model_name='gpt-4o',\n",
      "                        temperature=0.0,\n",
      "                        openai_api_key=SecretStr('**********'),\n",
      "                        openai_proxy='',\n",
      "                        azure_endpoint='https://mutualizedopenai.openai.azure.com',\n",
      "                        deployment_name='gpt-4o',\n",
      "                        openai_api_version='2023-05-15',\n",
      "                        openai_api_type='azure',\n",
      "                    ),\n",
      "                },\n",
      "                default_key='gpt_35_edenai',\n",
      "                prefix_keys=False,\n",
      "            ),\n",
      "        ],\n",
      "        last=StructuredQueryOutputParser(\n",
      "            ast_parse=<bound method Lark.parse of Lark(open('<string>'), parser='lalr', lexer='contextual', ...)>,\n",
      "        ),\n",
      "    ) (RunnableSequence)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['query'], examples=[{'i': 1, 'data_source': '```json\\n{{\\n    \"content\": \"Lyrics of a song\",\\n    \"attributes\": {{\\n        \"artist\": {{\\n            \"type\": \"string\",\\n            \"description\": \"Name of the song artist\"\\n        }},\\n        \"length\": {{\\n            \"type\": \"integer\",\\n            \"description\": \"Length of the song in seconds\"\\n        }},\\n        \"genre\": {{\\n            \"type\": \"string\",\\n            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"\\n        }}\\n    }}\\n}}\\n```', 'user_query': 'What are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genre', 'structured_request': '```json\\n{{\\n    \"query\": \"teenager love\",\\n    \"filter\": \"and(or(eq(\\\\\"artist\\\\\", \\\\\"Taylor Swift\\\\\"), eq(\\\\\"artist\\\\\", \\\\\"Katy Perry\\\\\")), lt(\\\\\"length\\\\\", 180), eq(\\\\\"genre\\\\\", \\\\\"pop\\\\\"))\"\\n}}\\n```'}, {'i': 2, 'data_source': '```json\\n{{\\n    \"content\": \"Lyrics of a song\",\\n    \"attributes\": {{\\n        \"artist\": {{\\n            \"type\": \"string\",\\n            \"description\": \"Name of the song artist\"\\n        }},\\n        \"length\": {{\\n            \"type\": \"integer\",\\n            \"description\": \"Length of the song in seconds\"\\n        }},\\n        \"genre\": {{\\n            \"type\": \"string\",\\n            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"\\n        }}\\n    }}\\n}}\\n```', 'user_query': 'What are songs that were not published on Spotify', 'structured_request': '```json\\n{{\\n    \"query\": \"\",\\n    \"filter\": \"NO_FILTER\"\\n}}\\n```'}], example_prompt=PromptTemplate(input_variables=['data_source', 'i', 'structured_request', 'user_query'], template='<< Example {i}. >>\\nData Source:\\n{data_source}\\n\\nUser Query:\\n{user_query}\\n\\nStructured Request:\\n{structured_request}\\n'), suffix='<< Example 3. >>\\nData Source:\\n```json\\n{{\\n    \"content\": \"Brief summary of a movie\",\\n    \"attributes\": {{\\n    \"genre\": {{\\n        \"description\": \"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",\\n        \"type\": \"string\"\\n    }},\\n    \"year\": {{\\n        \"description\": \"The year the movie was released\",\\n        \"type\": \"integer\"\\n    }},\\n    \"director\": {{\\n        \"description\": \"The name of the movie director\",\\n        \"type\": \"string\"\\n    }},\\n    \"rating\": {{\\n        \"description\": \"A 1-10 rating for the movie\",\\n        \"type\": \"float\"\\n    }}\\n}}\\n}}\\n```\\n\\nUser Query:\\n{query}\\n\\nStructured Request:\\n', prefix='Your goal is to structure the user\\'s query to match the request schema provided below.\\n\\n<< Structured Request Schema >>\\nWhen responding use a markdown code snippet with a JSON object formatted in the following schema:\\n\\n```json\\n{{\\n    \"query\": string \\\\ text string to compare to document contents\\n    \"filter\": string \\\\ logical condition statement for filtering documents\\n}}\\n```\\n\\nThe query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.\\n\\nA logical condition statement is composed of one or more comparison and logical operation statements.\\n\\nA comparison statement takes the form: `comp(attr, val)`:\\n- `comp` (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparator\\n- `attr` (string):  name of attribute to apply the comparison to\\n- `val` (string): is the comparison value\\n\\nA logical operation statement takes the form `op(statement1, statement2, ...)`:\\n- `op` (and | or | not): logical operator\\n- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation to\\n\\nMake sure that you only use the comparators and logical operators listed above and no others.\\nMake sure that filters only refer to attributes that exist in the data source.\\nMake sure that filters only use the attributed names with its function names if there are functions applied on them.\\nMake sure that filters only use format `YYYY-MM-DD` when handling date data typed values.\\nMake sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\\nMake sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.')\n",
       "| RunnableConfigurableAlternatives(default=ChatEdenAI(model='gpt-3.5-turbo-0125', max_tokens=2048, edenai_api_key=SecretStr('**********')), which=ConfigurableField(id='llm', name=None, description=None, annotation=None, is_shared=False), alternatives={'gpt_35_openai': ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f76ce2770d0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f76ce2773d0>, model_name='gpt-3.5-turbo-0125', temperature=0.0, model_kwargs={'seed': 42}, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=2048), 'gpt_4o_openai': ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f76cdf04fa0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f76ce29c190>, model_name='gpt-4o', temperature=0.0, model_kwargs={'seed': 42}, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=2048), 'llama3_70_deepinfra': ChatDeepInfra(name='meta-llama/Meta-Llama-3-70B-Instruct', deepinfra_api_token='rRHmU56sHzgr0bvZYbPYhogfCWTezIZT', model_kwargs={'temperature': 0, 'max_new_tokens': 2048, 'repetition_penalty': 1.3, 'stop': ['STOP_TOKEN']}), 'llama3_8_deepinfra': ChatDeepInfra(name='meta-llama/Meta-Llama-3-8B-Instruct', deepinfra_api_token='rRHmU56sHzgr0bvZYbPYhogfCWTezIZT', model_kwargs={'temperature': 0, 'max_new_tokens': 2048, 'repetition_penalty': 1.3, 'stop': ['STOP_TOKEN']}), 'mixtral_7x8_deepinfra': ChatDeepInfra(name='mistralai/Mixtral-8x7B-Instruct-v0.1', deepinfra_api_token='rRHmU56sHzgr0bvZYbPYhogfCWTezIZT', model_kwargs={'temperature': 0, 'max_new_tokens': 2048, 'repetition_penalty': 1.3, 'stop': ['STOP_TOKEN']}), 'llama3_70_deepinfra_lite': ChatLiteLLM(client=<module 'litellm' from '/home/tcl/.cache/pypoetry/virtualenvs/genai-training-2X6HL8i2-py3.10/lib/python3.10/site-packages/litellm/__init__.py'>, model='deepinfra/meta-llama/Llama-3-70b-chat-hf', openai_api_key='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', azure_api_key='', anthropic_api_key='', replicate_api_key='', cohere_api_key='', openrouter_api_key='', temperature=0.0, huggingface_api_key='', together_ai_api_key=''), 'llama3_70_groq': ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7f76cdf102e0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7f76cdf11360>, model_name='lLama3-70b-8192', temperature=1e-08, groq_api_key=SecretStr('**********'), max_tokens=2048), 'llama3_8_groq': ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7f76cdf13a30>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7f76ce3996f0>, model_name='lLama3-8b-8192', temperature=1e-08, groq_api_key=SecretStr('**********'), max_tokens=2048), 'mixtral_7x8_groq': ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7f76ce398640>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7f76ce39b910>, model_name='Mixtral-8x7b-32768', temperature=1e-08, groq_api_key=SecretStr('**********'), max_tokens=2048), 'llama3_8_local': ChatOllama(model='llama3:instruct', temperature=0.0), 'gpt_4o_edenai': ChatEdenAI(model='gpt-4o', max_tokens=2048, edenai_api_key=SecretStr('**********')), 'gpt_4_edenai': ChatEdenAI(model='gpt-4', max_tokens=2048, edenai_api_key=SecretStr('**********')), 'mistral_large_edenai': ChatEdenAI(provider='mistral', model='large-latest', max_tokens=2048, edenai_api_key=SecretStr('**********')), 'gpt_4_azure': AzureChatOpenAI(name='gpt4-turbo', client=<openai.resources.chat.completions.Completions object at 0x7f76ce39a080>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f76ce397580>, model_name='gpt4-turbo', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', azure_endpoint='https://mutualizedopenai.openai.azure.com', deployment_name='gpt4-turbo', openai_api_version='2023-05-15', openai_api_type='azure'), 'gpt_35_azure': AzureChatOpenAI(name='gpt-35-turbo', client=<openai.resources.chat.completions.Completions object at 0x7f76ce38a140>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f76ce437460>, model_name='gpt-35-turbo', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', azure_endpoint='https://mutualizedopenai.openai.azure.com', deployment_name='gpt-35-turbo', openai_api_version='2023-05-15', openai_api_type='azure'), 'gpt_4o_azure': AzureChatOpenAI(name='gpt-4o', client=<openai.resources.chat.completions.Completions object at 0x7f76cdf27370>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f76cdf25ea0>, model_name='gpt-4o', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', azure_endpoint='https://mutualizedopenai.openai.azure.com', deployment_name='gpt-4o', openai_api_version='2023-05-15', openai_api_type='azure')}, default_key='gpt_35_edenai', prefix_keys=False)\n",
       "| StructuredQueryOutputParser(ast_parse=<bound method Lark.parse of Lark(open('<string>'), parser='lalr', lexer='contextual', ...)>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = get_query_constructor(\n",
    "    {\n",
    "        \"query\": \"What are some sci-fi movies from the 90's directed by Luc Besson about taxi drivers\"\n",
    "    }\n",
    ")\n",
    "debug(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-20 19:17:24.197\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpython.ai_core.llm\u001b[0m:\u001b[36mget_llm\u001b[0m:\u001b[36m395\u001b[0m - \u001b[1mget LLM : mistral_large_edenai - configurable: True\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-20 19:17:24.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpython.ai_core.llm\u001b[0m:\u001b[36mget_configurable\u001b[0m:\u001b[36m351\u001b[0m - \u001b[1mCannot load gemini_pro_google: No module named 'langchain_google_vertexai'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"abbreviations.xlsx\", index_col=0)\n",
    "unknown = df[df.iloc[:, 0].isnull()]\n",
    "unknown_list = \"\\n\".join(unknown.index.tolist()[0:10])\n",
    "first = unknown.index.to_list()[0]\n",
    "\n",
    "\n",
    "system = \"\"\" \n",
    "Vous êtes expert du domaine de la formation et de l'enseignement supérieur.\n",
    "Repondez uniquement en Francais.\n",
    "Pour chacune des abréviations suivantes, completez par la signification si vous la connaissez.\n",
    "Si vous ne savez pas, afficher \"?\".\n",
    "N'ajoutez pas de commentaires et autres information.\n",
    "\"\"\"\n",
    "\n",
    "user = \"\"\"\n",
    "###  ABBREVIATIONS ###\n",
    "{abrev}\n",
    "### REPONSE: ###\n",
    "- {first_one}: ...\n",
    "- ...\n",
    "\"\"\"\n",
    "\n",
    "llm_mistral = get_llm(\"mistral_large_edenai\")\n",
    "\n",
    "prompt = def_prompt(system, user)\n",
    "chain = prompt | llm_mistral | StrOutputParser()\n",
    "\n",
    "# answer_mistral = chain.invoke({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-20 21:16:11.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpython.ai_core.llm\u001b[0m:\u001b[36mget_llm\u001b[0m:\u001b[36m395\u001b[0m - \u001b[1mget LLM : gpt_4o_edenai - configurable: True\u001b[0m\n",
      "\u001b[32m2024-06-20 21:16:11.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpython.ai_core.llm\u001b[0m:\u001b[36mget_configurable\u001b[0m:\u001b[36m351\u001b[0m - \u001b[1mCannot load gemini_pro_google: No module named 'langchain_google_vertexai'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "llm_gpt4 = get_llm(\"gpt_4o_edenai\")\n",
    "chain = prompt | llm_gpt4 | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- CAPPEI: Certificat d'Aptitude Professionnelle aux Pratiques de l'Éducation Inclusive\n",
      "- MPPM: ?\n",
      "- BIOGET: ?\n",
      "- MTS: ?\n",
      "- SIIA: ?\n",
      "- EIFFALE: ?\n",
      "- TROPIMUNDO: ?\n",
      "- TALN: Traitement Automatique du Langage Naturel\n",
      "- ESDD: ?\n",
      "- ORMS: ?\n"
     ]
    }
   ],
   "source": [
    "rep = chain.invoke({\"abrev\": unknown_list, \"first_one\": first})\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ESDD: Education au Développement Durable'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = \"- ESDD: Education au Développement Durable\"\n",
    "\n",
    "a = line.strip(\"- \")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- CAPPEI: Certificat d'Aptitude Professionnelle aux Pratiques de l'Education Inclusive\n",
      "- MPPM: ?\n",
      "- BIOGET: ?\n",
      "- MTS: Mastère Spécialisé\n",
      "- SIIA: Systèmes d'Information et d'Ingénierie Avancée\n",
      "- EIFFALE: ?\n",
      "- TROPIMUNDO: ?\n",
      "- TALN: Traitement Automatique des Langues Naturelles\n",
      "- ESDD: Education au Développement Durable\n",
      "- ORMS: ?\n"
     ]
    }
   ],
   "source": [
    "lines = rep.split(\"\\n\")\n",
    "\n",
    "d = dict()\n",
    "for line in lines:\n",
    "    print(line)\n",
    "    k, _, v = line.partition(\":\")\n",
    "    d |= {k.strip(\"- \"): v.strip()}\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_dict(d, orient=\"index\")\n",
    "df.to_excel(\"abbreviations2.xlsx\", sheet_name=\"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abbrev': 'ORMS', 'desc': '?'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "l = [1, 1]\n",
    "for x in l or []:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 2]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l += [2, 2]\n",
    "l"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "di-maintenance-agent-demo-DCv9_gKb-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
