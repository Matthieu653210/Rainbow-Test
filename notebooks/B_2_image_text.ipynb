{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image to Text with LCEL\n",
    "### (with GPT-4o and maybe others)\n",
    "\n",
    "Inspired by: https://tykimos.github.io/2024/05/15/image_descriptions_with_gpt_4o_and_lcel/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from python.ai_core.llm import get_llm\n",
    "\n",
    "load_dotenv(verbose=True)\n",
    "\n",
    "!export PYTHONPATH=\":./python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Chain to query an image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(param_dict: dict) -> list[BaseMessage]:\n",
    "    # Function to generate a prompt based on given parameters\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant that kindly explains images and answers questions provided by the user.\"\n",
    "    )\n",
    "    human_messages = [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"{param_dict['question']}\",\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"{param_dict['image_url']}\",\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "    return [SystemMessage(content=system_message), HumanMessage(content=human_messages)]\n",
    "\n",
    "\n",
    "llm = get_llm(llm_id=\"gpt_4o_openai\")\n",
    "# Does not work;\n",
    "# llm = get_llm(llm_id=\"gpt_4o_edenai\")\n",
    "# llm = get_llm(llm_id=\"gpt_4_azure\")\n",
    "llm = get_llm(llm_id=\"llava_phi3_ollama\")\n",
    "chain = gen_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed the image in the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE_PATH = \"use_case_data/railway/network rail.png\"\n",
    "\n",
    "\n",
    "REPO = Path(\"/mnt/c/Users/a184094/OneDrive - Eviden/_ongoing/training GenAI/\")\n",
    "IMAGE_PATH = REPO / \"network rail.png\"\n",
    "\n",
    "\n",
    "def encode_image(image_path: Path) -> str:\n",
    "    # Open the image file and encode it as a base64 string\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "base64_image = encode_image(IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"question\": \"Please describe this junction.\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "    }\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Langchain API ?  \n",
    "Seems to work for  Ollama only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_image_context = llm.bind(images=[base64_image])  # image_b64 is your base64 encoded image\n",
    "response = llm_with_image_context.invoke(\"Please describe this rail junction.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** EXPERIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import requests\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {os.environ['EDENAI_API_KEY']}\"}\n",
    "url = \"https://api.edenai.run/v2/multimodal/chat\"\n",
    "\n",
    "\n",
    "# Function to read the image file and convert it to base64\n",
    "with open(IMAGE_PATH, \"rb\") as image_file:\n",
    "    base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "payload = {\n",
    "    \"providers\": \"openai, google\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"content\": {\"text\": \"Describe this image please!\"},\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"media_base64\",\n",
    "                    \"content\": {\n",
    "                        \"media_base64\": base64_image,\n",
    "                        \"media_type\": \"image/png\",\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    \"chatbot_global_action\": \"\",\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "result = response.json()\n",
    "print(result[\"openai\"][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"google\"][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-blueprint-2X6HL8i2-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
