{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from devtools import debug\n",
    "\n",
    "[sys.path.append(str(path)) for path in [Path.cwd(), Path.cwd().parent, Path.cwd().parent/\"python\"] if str(path) not in sys.path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ChatLiteLLM' from 'langchain_community.chat_models' (/home/tcl/.cache/pypoetry/virtualenvs/genai-training-GvhVNXsu-py3.10/lib/python3.10/site-packages/langchain_community/chat_models/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatLiteLLM\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[1;32m      4\u001b[0m chat \u001b[38;5;241m=\u001b[39m ChatLiteLLM(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ChatLiteLLM' from 'langchain_community.chat_models' (/home/tcl/.cache/pypoetry/virtualenvs/genai-training-GvhVNXsu-py3.10/lib/python3.10/site-packages/langchain_community/chat_models/__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatLiteLLM\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat = ChatLiteLLM(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"Translate this sentence from English to French. I love programming.\"\n",
    "    )\n",
    "]\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'), HumanMessage(content='Hello, how are you doing?'), AIMessage(content=\"I'm doing well, thanks!\"), HumanMessage(content='What is your name?')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"user\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"user\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'), HumanMessage(content='Hello, how are you doing?'), AIMessage(content=\"I'm doing well, thanks!\"), HumanMessage(content='What is your name?'), HumanMessage(content='Summarize our conversation so far in 10 words.'), HumanMessage(content='Summarize our conversation so far in 10 words.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "from langchain_experimental.chat_models import Llama2Chat\n",
    "\n",
    "human_prompt = \"Summarize our conversation so far in {word_count} words.\"\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [MessagesPlaceholder(variable_name=\"conversation\"), human_message_template]\n",
    ")\n",
    "messages = chat_prompt.format_messages( word_count = 10, conversation = messages)\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, cast\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    ChatGeneration,\n",
    "    ChatResult,\n",
    "    HumanMessage,\n",
    "    LLMResult,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\n",
    "\n",
    "class PromptFormatter(BaseModel):\n",
    "\n",
    "    system_message: SystemMessage = SystemMessage(content=DEFAULT_SYSTEM_PROMPT)\n",
    "\n",
    "    text_beg : str = \"\"\n",
    "    sys_beg: str\n",
    "    sys_end: str\n",
    "    ai_n_beg: str\n",
    "    ai_n_end: str\n",
    "    usr_n_beg: str\n",
    "    usr_n_end: str\n",
    "    usr_0_beg: Optional[str] = None\n",
    "    usr_0_end: Optional[str] = None\n",
    "    text_end : str =\"\"\n",
    "\n",
    "    def _to_chat_prompt(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "    ) -> str:\n",
    "        \"\"\"Convert a list of messages into a prompt format expected by wrapped LLM.\"\"\"\n",
    "        if not messages:\n",
    "            raise ValueError(\"at least one HumanMessage must be provided\")\n",
    "\n",
    "        if not isinstance(messages[0], SystemMessage):\n",
    "            messages = [self.system_message] + messages\n",
    "\n",
    "        if not isinstance(messages[1], HumanMessage):\n",
    "            raise ValueError(\n",
    "                \"messages list must start with a SystemMessage or UserMessage\"\n",
    "            )\n",
    "\n",
    "        if not isinstance(messages[-1], HumanMessage):\n",
    "            raise ValueError(\"last message must be a HumanMessage\")\n",
    "\n",
    "        prompt_parts = [self.text_beg]\n",
    "\n",
    "        if self.usr_0_beg is None:\n",
    "            self.usr_0_beg = self.usr_n_beg\n",
    "\n",
    "        if self.usr_0_end is None:\n",
    "            self.usr_0_end = self.usr_n_end\n",
    "\n",
    "        prompt_parts.append(\n",
    "            self.sys_beg + cast(str, messages[0].content) + self.sys_end\n",
    "        )\n",
    "        prompt_parts.append(\n",
    "            self.usr_0_beg + cast(str, messages[1].content) + self.usr_0_end\n",
    "        )\n",
    "\n",
    "        for ai_message, human_message in zip(messages[2::2], messages[3::2]):\n",
    "            prompt_parts.append(\n",
    "                self.ai_n_beg + cast(str, ai_message.content) + self.ai_n_end\n",
    "            )\n",
    "            prompt_parts.append(\n",
    "                self.usr_n_beg + cast(str, human_message.content) + self.usr_n_end\n",
    "            )\n",
    "\n",
    "        prompt_parts.append(self.text_end)\n",
    "        return \"\".join(prompt_parts)\n",
    "\n",
    "\n",
    "class Llama2Format(PromptFormatter):\n",
    "    \"\"\"Wrapper for Llama-2-chat model.\"\"\"\n",
    "    # \n",
    "\n",
    "    sys_beg: str = \"<s>[INST] <<SYS>>\\n\"\n",
    "    sys_end: str = \"\\n<</SYS>>\\n\\n\"\n",
    "    ai_n_beg: str = \" \"\n",
    "    ai_n_end: str = \" </s>\"\n",
    "    usr_n_beg: str = \"<s>[INST] \"\n",
    "    usr_n_end: str = \" [/INST]\"\n",
    "    usr_0_beg: str = \"\"\n",
    "    usr_0_end: str = \" [/INST]\"\n",
    "\n",
    "class Llama3Format(PromptFormatter):\n",
    "    \"\"\"Wrapper for Llama-2-chat model.\"\"\"\n",
    "\n",
    "    text_beg: str = \"<|begin_of_text|>\"\n",
    "    sys_beg: str = \"<|start_header_id|>system<|end_header_id|>\"\n",
    "    sys_end: str = \"<|eot_id|>\\n\"\n",
    "    ai_n_beg: str = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    ai_n_end: str = \"<|eot_id|>\"\n",
    "    usr_n_beg: str = \"<|start_header_id|>user<|end_header_id|>\"\n",
    "    usr_n_end: str = \"<|eot_id|>\"\n",
    "    text_end: str = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "\n",
    "\n",
    "class MixtralFormat(PromptFormatter):\n",
    "    \"\"\"See https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1#instruction-format\"\"\"  # noqa: E501\n",
    "\n",
    "    # https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/#meta-llama-3-chat\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"mixtral\"\n",
    "\n",
    "    sys_beg: str = \"<s>[INST] \"\n",
    "    sys_end: str = \"\\n\"\n",
    "    ai_n_beg: str = \" \"\n",
    "    ai_n_end: str = \" </s>\"\n",
    "    usr_n_beg: str = \" [INST] \"\n",
    "    usr_n_end: str = \" [/INST]\"\n",
    "    usr_0_beg: str = \"\"\n",
    "    usr_0_end: str = \" [/INST]\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>system message<|eot_id|><|start_header_id|>user<|end_header_id|>user message<|eot_id|><|start_header_id|>assistant<|end_header_id|>'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "formatter = Llama3Format()\n",
    "formatter._to_chat_prompt(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' <|begin_of_text|><|start_header_id|>system<|end_header_id|> system message <|eot_id|>\\n    <|start_header_id|>user<|end_header_id|>user message <|eot_id|><|start_header_id|>assistant<|end_header_id|>'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_original = PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> system message <|eot_id|>\n",
    "    <|start_header_id|>user<|end_header_id|>user message <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[],\n",
    ")\n",
    "prompt_original.format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>system message<|eot_id|><|start_header_id|>user<|end_header_id|>user message<|eot_id|>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"system message\"),\n",
    "        (\"user\", \"user message\"),\n",
    "    ]\n",
    ")\n",
    "messages = new_prompt.format_messages()\n",
    "formatter._to_chat_prompt(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_322598/1230483511.py:30 to_chat_prompt\n",
      "    input_prompt: ChatPromptTemplate(\n",
      "        input_variables=['topic'],\n",
      "        messages=[\n",
      "            SystemMessagePromptTemplate(\n",
      "                prompt=PromptTemplate(\n",
      "                    input_variables=['topic'],\n",
      "                    template='this is a system message talking about {topic}',\n",
      "                ),\n",
      "            ),\n",
      "            HumanMessagePromptTemplate(\n",
      "                prompt=PromptTemplate(\n",
      "                    input_variables=[],\n",
      "                    template=(\n",
      "                        '\\n'\n",
      "                        '            This is a long\\n'\n",
      "                        '            user message'\n",
      "                    ),\n",
      "                ),\n",
      "            ),\n",
      "        ],\n",
      "    ) (ChatPromptTemplate) len=2\n",
      "/tmp/ipykernel_322598/1230483511.py:32 to_chat_prompt\n",
      "    prompt.input_variables: [\n",
      "        'documents',\n",
      "        'generation',\n",
      "    ] (list) len=2\n",
      "/tmp/ipykernel_322598/1230483511.py:32 to_chat_prompt\n",
      "    prompt.input_variables: [\n",
      "        'documents',\n",
      "        'generation',\n",
      "    ] (list) len=2\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "from typing import List, Optional, cast\n",
    "from langchain_core.prompts import ChatPromptTemplate, BasePromptTemplate\n",
    "from langchain_core.prompts import (\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain.schema import (\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "from pydantic import BaseModel\n",
    "from devtools import debug\n",
    "\n",
    "\n",
    "\n",
    "def to_chat_prompt(\n",
    "    input_prompt: ChatPromptTemplate,\n",
    ") :\n",
    "    \n",
    "    \"\"\"Convert a list of messages into a prompt format expected by wrapped LLM.\"\"\"\n",
    "\n",
    "\n",
    "    template = []\n",
    "    input_variables = []\n",
    "\n",
    "    debug(input_prompt)\n",
    "    for message in input_prompt.messages:\n",
    "        if isinstance(message, SystemMessagePromptTemplate):\n",
    "            template.append(\n",
    "                \"self.sys_beg\" + dedent(cast(str, message.prompt.template) + \"self.sys_end\"\n",
    "            ))\n",
    "        elif \n",
    "\n",
    "\n",
    "new_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"this is a system message talking about {topic}\"\"\"),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"\"\"\n",
    "            This is a long\n",
    "            user message\"\"\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "to_chat_prompt(new_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "di-maintenance-agent-demo-DCv9_gKb-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
